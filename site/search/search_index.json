{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Revisiting normalizing flows Fig 1: Samples from an unconditional model with affine coupling layers trained on the CIFAR-10 dataset with temperature 1.0 after 10 epochs ( left ) and 600 epochs ( right ) using Horovod (You can find our Dockerfile here ). If you recall from our previous discussion on Glow ( Kingma and Dhariwal 2018 ), an attractive aspect of normalizing flows is that the exact log-likelihood of input data \\log p(\\mathbf{x}) becomes tractable. As a result, the training criterion of a flow-based generative model is simply the negative log-likelihood (bits per dimension) over the training dataset \\mathcal{D} : \\mathcal{L}(\\mathcal{D}) = - \\frac{1}{\\vert\\mathcal{D}\\vert}\\sum_{\\mathbf{x} \\in \\mathcal{D}} \\log p(\\mathbf{x}) Affine coupling layers As I mentioned, in the RealNVP (Real-valued Non-Volume Preserving; Dinh et al. 2017 ) paper normalizing flows are implemented by stacking a sequence of invertible bijective transformation functions. In each bijection f: \\mathbf{x} \\mapsto \\mathbf{y} , known as the affine coupling layer (ACL), the input dimensions are split into two parts: The first d dimensions remain the same. The second part, d+1 to D dimensions, undergo an affine transformation (\u201cscale-and-shift\u201d) and both the scale and shift parameters are functions of the first d dimensions. % <![CDATA[ \\begin{aligned} \\mathbf{y}_{1:d} &= \\mathbf{x}_{1:d} \\\\ \\mathbf{y}_{d+1:D} &= \\mathbf{x}_{d+1:D} \\odot \\exp({s(\\mathbf{x}_{1:d})}) + t(\\mathbf{x}_{1:d}) \\end{aligned} %]]> where s(.) and t(.) are scale and translation functions and both map \\mathbb{R}^d \\mapsto \\mathbb{R}^{D-d} . How would we go about coding this? Coding bijective functions The first component of the input dimension is straightforward since it's simply: y [ 0 , d ] = x [ 0 : d ] However, the second component requires that we scale-and-shift the last D-d units contingent on the first d units only, while the first d units are masked and left unchanged. In TensorFlow, this would look like: y [ d : D ] = x [ d : D ] * tf . exp ( log_scale_fn ( x [ 0 : d ])) + shift_fn ( x [ 0 : d ]) More to come soon...","title":"Latest"},{"location":"#revisiting-normalizing-flows","text":"Fig 1: Samples from an unconditional model with affine coupling layers trained on the CIFAR-10 dataset with temperature 1.0 after 10 epochs ( left ) and 600 epochs ( right ) using Horovod (You can find our Dockerfile here ). If you recall from our previous discussion on Glow ( Kingma and Dhariwal 2018 ), an attractive aspect of normalizing flows is that the exact log-likelihood of input data \\log p(\\mathbf{x}) becomes tractable. As a result, the training criterion of a flow-based generative model is simply the negative log-likelihood (bits per dimension) over the training dataset \\mathcal{D} : \\mathcal{L}(\\mathcal{D}) = - \\frac{1}{\\vert\\mathcal{D}\\vert}\\sum_{\\mathbf{x} \\in \\mathcal{D}} \\log p(\\mathbf{x})","title":"Revisiting normalizing flows"},{"location":"#affine-coupling-layers","text":"As I mentioned, in the RealNVP (Real-valued Non-Volume Preserving; Dinh et al. 2017 ) paper normalizing flows are implemented by stacking a sequence of invertible bijective transformation functions. In each bijection f: \\mathbf{x} \\mapsto \\mathbf{y} , known as the affine coupling layer (ACL), the input dimensions are split into two parts: The first d dimensions remain the same. The second part, d+1 to D dimensions, undergo an affine transformation (\u201cscale-and-shift\u201d) and both the scale and shift parameters are functions of the first d dimensions. % <![CDATA[ \\begin{aligned} \\mathbf{y}_{1:d} &= \\mathbf{x}_{1:d} \\\\ \\mathbf{y}_{d+1:D} &= \\mathbf{x}_{d+1:D} \\odot \\exp({s(\\mathbf{x}_{1:d})}) + t(\\mathbf{x}_{1:d}) \\end{aligned} %]]> where s(.) and t(.) are scale and translation functions and both map \\mathbb{R}^d \\mapsto \\mathbb{R}^{D-d} . How would we go about coding this?","title":"Affine coupling layers"},{"location":"#coding-bijective-functions","text":"The first component of the input dimension is straightforward since it's simply: y [ 0 , d ] = x [ 0 : d ] However, the second component requires that we scale-and-shift the last D-d units contingent on the first d units only, while the first d units are masked and left unchanged. In TensorFlow, this would look like: y [ d : D ] = x [ d : D ] * tf . exp ( log_scale_fn ( x [ 0 : d ])) + shift_fn ( x [ 0 : d ]) More to come soon...","title":"Coding bijective functions"},{"location":"02-22-19-glow/","text":"","title":"Glow: Generative flow with invertible 1x1 convolutions"},{"location":"03-22-19-gcn/","text":"","title":"Graph convolutional networks"},{"location":"11-12-18-microrna/","text":"","title":"MicroRNA targeting efficacy"},{"location":"about/","text":"The Machine Learning group is a research team within Simulation and Modeling Sciences . As part of R&D, the team focuses on developing theory and methods in machine learning with the goal of solving difficult, yet tractable problems in drug discovery. We have interests at the intersection of computational sciences, Bayesian statistics, and machine learning.","title":"Mission"},{"location":"de-novo/","text":"","title":"De novo molecule generation"},{"location":"dgms/","text":"","title":"DGMs"},{"location":"publications/","text":"Deep learning of representations for transcriptomics-based phenotype prediction Aaron M. Smith, Jonathan R. Walsh, John Long, Craig B. Davis, Peter Henstock, Martin R. Hodge, Mateusz Maciejewski, Xinmeng Jasminue Mu, Stephen Ra, Shanrong Zhao, Daniel Ziemek, Charles K. Fisher bioRxiv , 2019. doi:10.1101/574723 Paper Deep generative models for the directed expansion of compound libraries Vishnu Sresht, Stephen Ra, Brajesh Rai, Bruce A. Lefker, Alan Mathiowetz American Chemical Society (ACS) National Meeting 2018 , 2012; 17, 1054-55. doi:10.1038/mp.2012.71 Paper","title":"Publications"},{"location":"team/","text":"","title":"Team"}]}