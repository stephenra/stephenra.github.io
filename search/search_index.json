{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"On conditioning by adaptive sampling (Part I) A recent paper by David Brookes, Hahnbeom Park, and Jennifer Listgarten 1 extends their work from last year that posits a framework, Conditioning by Adaptive Sampling (CbAS) , for model-based conditional density estimation. Of particular relevance to us here at Pfizer is the application to protein design. For example, how do we go about optimizing the design of DNA sequences for protein expression, binding, or for protein sequences that maximize properties like secondary structure or fluorescence? Traditionally, in vitro methods for protein engineering have been exhaustive usually involving greedy walks through protein design space like directed evolution (DE) . This paper proposes a more efficient in silico approach. For this particular entry, I'll summarize the key contributions and introduce some of the core concepts. The subsequent post will walk through the Cross-Entropy Method (CEM) and their experimental results. Summary The main contributions of this paper are as follows: The authors present a method for tackling design problems in discrete space including the problem of maximizing protein fluorescence using avGFP sequence data from Sarkisyan et al. 2016 . A principled approach for conditional density estimation in the presence of rare events and which doesn't constrain the oracle by requiring differentiability. This has the added benefit of recapitulating models in various scientific domains which may not be readily differentiable to begin with It also allows for a degree of flexibility of validation by using wet-lab experiments as the oracle. An exact and iterative importance sampling-based estimation method for variance reduction and computationally intractable model densities. Generalizability of CbAS to specification of property values rather than solely maximization (e.g. a protein fluorescing at a desired wavelength as opposed to being maximally fluorescent). Oracles and oracle properties In order to understand the problem setting in the paper, it's worth briefly reviewing oracles. In statistic learning, oracle and oracle properties have been extensively used in the context of high-dimensional model estimation and feature selection whereby we have a response vector \\boldsymbol{y} = (y_1,...,y_n)^{T} and input vector of linearly independent features \\boldsymbol{x_j} = (x_{1j},...,x_{nj})^{T} with j = 1,..,p . Under the assumption that we have \\mathbb{A} = \\{ j: \\hat{\\beta} \\neq 0 \\} , the true model only depends on a subset X_{S} \\subset X of its feature space. Briefly put, an oracle property is a theoretical attribute ascribed to a model if it performs as well as if the correct underlying model were given in advance. More formally and following Fan and Li 2001 , Fan and Peng 2004 , and Zou 2006 , we call a given fitting procedure \\delta an oracle if \\hat{\\beta}(\\delta) , the coefficient estimator, identifies the right subset model, \\{ j: \\hat{\\beta} \\neq 0 \\} = \\mathbb{A} with some optimal estimation rate: \\sqrt{n} \\hat{\\beta}(\\delta)_{\\mathbb{A}} - \\beta^*_{\\mathbb{A}}) \\to \\mathcal{N} (0, I^{-1}(\\beta^*_\\mathbb{A})) Thus, the oracle property holds that the asymptotic distribution of the estimator is the same as the asymptotic distribution of the MLE only on the true support. That is, the estimator adapts to knowing the true support without incurring a price. Why is this important? For the authors, they argue that their oracle p(y|\\boldsymbol{x}) effectively serves as a proxy for expensive and exhaustive laboratory-based property measurements. They also make the assumption that given an input, such as a DNA sequence, the oracle 2 returns a distribution over the properties of interest. It's worth noting that this is within the context of the generative setting where we model the joint probability distribution p(\\boldsymbol{x}, y) . Given the property oracle then, one can subsequently model and empirically sample from the underlying data distribution p_{d}(\\boldsymbol{x}) with a generative model p(\\boldsymbol{x}|\\theta) whose prior is conditioned on the desiderata encoded in property values S . The claim here is that this gives a certain degree of flexibility in specifying priors on the design space, unlike G\u00f3mez-Bombarelli et al. 2018 and Killoran et al. 2017 , which utilize a fixed VAE and GAN respectively. Lastly, it's worth noting that they do not require their oracle to be differentiable and this additionally circumvents backpropagation through discrete input space. Conditioning by adaptive sampling The problem setting is as follows: find settings of the discrete random vector \\boldsymbol{x} \\in \\mathbb{N}^{L} (e.g. representative of DNA sequences) that have a high probability of satisfying some property desiderata (e.g. maximal fluoresence of a protein). The inference problem is expressed as: p(\\boldsymbol{x}|S, \\theta^{(0)}) = \\frac{P(S|\\boldsymbol{x})p(\\boldsymbol{x}|\\theta^{(0)})}{P(S|\\theta^{(0)})} p(\\boldsymbol{x}|S, \\theta^{(0)}) = \\frac{P(S|\\boldsymbol{x})p(\\boldsymbol{x}|\\theta^{(0)})}{P(S|\\theta^{(0)})} where P(S|\\theta^{(0)}) = \\int_{s} P(S|\\boldsymbol{x})p(\\boldsymbol{x}|\\theta^{(0)}) d\\boldsymbol{x} . Like variational inference , the marginal probability of CbAS yields no analytic solution as the sum over all possible configurations of the latent structures and its parameters is intractable. Unsurprisingly, inference for CbAS is VI-like in that they minimize the KL divergence between the conditional and the approximating search model, q(\\boldsymbol{x}|\\phi) : \\phi^{*} = \\underset{\\phi}{\\operatorname{argmin}} D_{KL} \\big(p(\\boldsymbol{x}|S,\\theta^{(0)})||q(\\boldsymbol{x}|\\phi\\big) \\phi^{*} = \\underset{\\phi}{\\operatorname{argmin}} D_{KL} \\big(p(\\boldsymbol{x}|S,\\theta^{(0)})||q(\\boldsymbol{x}|\\phi\\big) This can be re-written in terms of the log expectation of the difference between the search model distribution and the target conditional distribution, similar to how the Evidence Lower Bound (ELBO) can be derived: \\underset{\\phi}{\\operatorname{argmin}}\\mathbb{E}_{p(\\boldsymbol{x}|S,\\theta^{(0)})}[\\log p(\\boldsymbol{x}|S,\\theta^{(0)}) - \\log q(\\boldsymbol{x}|\\phi)] \\underset{\\phi}{\\operatorname{argmin}}\\mathbb{E}_{p(\\boldsymbol{x}|S,\\theta^{(0)})}[\\log p(\\boldsymbol{x}|S,\\theta^{(0)}) - \\log q(\\boldsymbol{x}|\\phi)] Thus, the second expectation can be expressed as the entropy of the target conditional distribution H_{0} \\equiv -\\mathbb{E}_{p(\\boldsymbol{x}|S,\\theta^{(0)})}[\\log p(\\boldsymbol{x}|S,\\theta^{(0)})] : \\begin{align} & = \\underset{\\phi}{\\operatorname{argmin}}-\\mathbb{E}_{p(\\boldsymbol{x}|S,\\theta^{(0)})}[\\log q(\\boldsymbol{x}|\\phi)] - H_{0} \\newline & = \\underset{\\phi}{\\operatorname{argmax}} \\frac{1}{P(S|\\theta^{(0)})} \\mathbb{E}_{(p(\\boldsymbol{x}|S,\\theta^{(0)})}[P(S|\\boldsymbol{x}) \\log q(\\boldsymbol{x}|\\phi)] \\newline & = \\underset{\\phi}{\\operatorname{argmax}}\\mathbb{E}_{p(\\boldsymbol{x}|\\theta^{(0)})}[P(S|\\boldsymbol{x}) \\log q(\\boldsymbol{x}|\\phi)] & \\scriptstyle{H_{0}, P(S|\\theta^{(0)}) \\text{ dropped}} \\\\[2mm] \\end{align} The authors also note that the difficulty of estimating the normalizing constant is compounded by the fact that S (and by definition, P(S|\\boldsymbol{x}) ) is \"exceedingly rare\". This translates to difficulties in optimization: any gradient-based reparameterization which yields differentiable Monte Carlo (MC) estimates of our expectation (i.e. the 'reparameterization trick' from Kingma and Welling 2014 ) would require a non-trivial number of samples to be accurate and be very challenging for distributions over discrete state spaces. To put it another way, any naive MC estimation would require an intensive simulation effort, inversely proportional to the magnitude of P(S|\\boldsymbol{x}) . Importance sampling-based estimation I'm not going to exhaustively review importance sampling here 3 . Suffice to say that the basic idea is that we can estimate the expectation of a random variable under one distribution p(\\boldsymbol{x}) from samples of a different distribution r(\\boldsymbol{x}) and that our importance sampling distribution r(\\boldsymbol{x}) can be used to mitigate the problem of high variance and an exceedingly small expectation \\mathbb{E}_{r^{(t)}(\\boldsymbol{x})} \\big[P(S^{(t)}|\\boldsymbol{x}\\big] . This approach is not without drawbacks however, and perturbations in weight space can have deleterious effects on the estimator. Thus, the objective can be rewritten as: \\begin{align} & = \\mathbb{E}_{r(\\boldsymbol{x})}\\Bigg[\\frac{p(\\boldsymbol{x}|\\theta^{(0)})}{r(x)}[P(S|\\boldsymbol{x}) \\log q(\\boldsymbol{x}|\\phi)]\\Bigg] \\newline & = \\mathbb{E}_{q(\\boldsymbol{x}|\\phi^{(t)})}\\Bigg[\\frac{p(\\boldsymbol{x}|\\theta^{(0)})}{q(\\boldsymbol{x}|\\phi^{(t)})}[P(S^{(t)}|\\boldsymbol{x}) \\log q(\\boldsymbol{x}|\\phi)]\\Bigg] \\end{align} where p(\\boldsymbol{x}|\\theta^{(0)}) is our target density, q(\\boldsymbol{x}|\\phi^{(t)}) is our proposed density and the ratio of the two: \\frac{p(\\boldsymbol{x}|\\theta^{(0)})}{q(\\boldsymbol{x}|\\phi^{(t)})} \\frac{p(\\boldsymbol{x}|\\theta^{(0)})}{q(\\boldsymbol{x}|\\phi^{(t)})} is the eponymous importance weight . Now that we have this objective and given samples \\{x^{(i)} \\}^{(n)}_{i} from q(\\boldsymbol{x}|\\phi^{(t)}) , we can construct MC estimates: \\begin{align} & \\phi^{(t+1)} = \\underset{\\phi}{\\operatorname{argmin}}\\sum_{i=1}^{M} \\frac{p(\\boldsymbol{x}_{i}^{(t)}|\\theta^{(0)})}{q(\\boldsymbol{x}_{i}^{(t)}|\\phi^{(t)})} P(S^{(t)}|\\boldsymbol{x}_{i}^{(t)}) \\log q(\\boldsymbol{x}_{i}^{(t)}|\\phi)] \\end{align} I think this is a particularly clever reformulation because it does not constrain the authors' choice of search model distribution to a single model class. For example, for cases where marginalization over latent space is intractable and the CbAS objective cannot be solved exactly (i.e. where a vanilla VAE is used), approximate inference approaches can be used instead to extend the model class, something I alluded to earlier. I'm not going to go through the explicit derivation of the ELBO-like objective but it's worth noting that in many cases where the CbAS update can be solved, this product: \\begin{align} \\require{color} & \\phi^{(t+1)} = \\underset{\\phi}{\\operatorname{argmin}}\\sum_{i=1}^{M} \\colorbox{GreenYellow}{$\\frac{p(\\boldsymbol{x}_{i}^{(t)}|\\theta^{(0)})}{q(\\boldsymbol{x}_{i}^{(t)}|\\phi^{(t)})} P(S^{(t)}|\\boldsymbol{x}_{i}^{(t)})$} \\log q(\\boldsymbol{x}_{i}^{(t)}|\\phi)] \\end{align} translates to an importance weighting of the probability of a series of property value sets S^{(t)} conditioned on each data point, \\boldsymbol{x}_{i}^{(t)} (i.e. the weight of each data point). This is similar to a weighted maximum likelihood objective and can therefore be optimized using any number of standard approaches for training generative models. More to come on the experimental results in the next entry... A good entry point to Listgarten's earlier work is her interview on Talking Machines , where she discusses her work at MSR New England on predicting off-target effects for CRISPR guide design. \u21a9 Additional assumptions are that the oracle is well-behaved and noise-free \u21a9 For this, I recommend this chapter from Art Owen's book, \"Monte Carlo theory, methods and examples\". \u21a9","title":"Latest"},{"location":"#on-conditioning-by-adaptive-sampling-part-i","text":"A recent paper by David Brookes, Hahnbeom Park, and Jennifer Listgarten 1 extends their work from last year that posits a framework, Conditioning by Adaptive Sampling (CbAS) , for model-based conditional density estimation. Of particular relevance to us here at Pfizer is the application to protein design. For example, how do we go about optimizing the design of DNA sequences for protein expression, binding, or for protein sequences that maximize properties like secondary structure or fluorescence? Traditionally, in vitro methods for protein engineering have been exhaustive usually involving greedy walks through protein design space like directed evolution (DE) . This paper proposes a more efficient in silico approach. For this particular entry, I'll summarize the key contributions and introduce some of the core concepts. The subsequent post will walk through the Cross-Entropy Method (CEM) and their experimental results.","title":"On conditioning by adaptive sampling (Part I)"},{"location":"#summary","text":"The main contributions of this paper are as follows: The authors present a method for tackling design problems in discrete space including the problem of maximizing protein fluorescence using avGFP sequence data from Sarkisyan et al. 2016 . A principled approach for conditional density estimation in the presence of rare events and which doesn't constrain the oracle by requiring differentiability. This has the added benefit of recapitulating models in various scientific domains which may not be readily differentiable to begin with It also allows for a degree of flexibility of validation by using wet-lab experiments as the oracle. An exact and iterative importance sampling-based estimation method for variance reduction and computationally intractable model densities. Generalizability of CbAS to specification of property values rather than solely maximization (e.g. a protein fluorescing at a desired wavelength as opposed to being maximally fluorescent).","title":"Summary"},{"location":"#oracles-and-oracle-properties","text":"In order to understand the problem setting in the paper, it's worth briefly reviewing oracles. In statistic learning, oracle and oracle properties have been extensively used in the context of high-dimensional model estimation and feature selection whereby we have a response vector \\boldsymbol{y} = (y_1,...,y_n)^{T} and input vector of linearly independent features \\boldsymbol{x_j} = (x_{1j},...,x_{nj})^{T} with j = 1,..,p . Under the assumption that we have \\mathbb{A} = \\{ j: \\hat{\\beta} \\neq 0 \\} , the true model only depends on a subset X_{S} \\subset X of its feature space. Briefly put, an oracle property is a theoretical attribute ascribed to a model if it performs as well as if the correct underlying model were given in advance. More formally and following Fan and Li 2001 , Fan and Peng 2004 , and Zou 2006 , we call a given fitting procedure \\delta an oracle if \\hat{\\beta}(\\delta) , the coefficient estimator, identifies the right subset model, \\{ j: \\hat{\\beta} \\neq 0 \\} = \\mathbb{A} with some optimal estimation rate: \\sqrt{n} \\hat{\\beta}(\\delta)_{\\mathbb{A}} - \\beta^*_{\\mathbb{A}}) \\to \\mathcal{N} (0, I^{-1}(\\beta^*_\\mathbb{A})) Thus, the oracle property holds that the asymptotic distribution of the estimator is the same as the asymptotic distribution of the MLE only on the true support. That is, the estimator adapts to knowing the true support without incurring a price. Why is this important? For the authors, they argue that their oracle p(y|\\boldsymbol{x}) effectively serves as a proxy for expensive and exhaustive laboratory-based property measurements. They also make the assumption that given an input, such as a DNA sequence, the oracle 2 returns a distribution over the properties of interest. It's worth noting that this is within the context of the generative setting where we model the joint probability distribution p(\\boldsymbol{x}, y) . Given the property oracle then, one can subsequently model and empirically sample from the underlying data distribution p_{d}(\\boldsymbol{x}) with a generative model p(\\boldsymbol{x}|\\theta) whose prior is conditioned on the desiderata encoded in property values S . The claim here is that this gives a certain degree of flexibility in specifying priors on the design space, unlike G\u00f3mez-Bombarelli et al. 2018 and Killoran et al. 2017 , which utilize a fixed VAE and GAN respectively. Lastly, it's worth noting that they do not require their oracle to be differentiable and this additionally circumvents backpropagation through discrete input space.","title":"Oracles and oracle properties"},{"location":"#conditioning-by-adaptive-sampling","text":"The problem setting is as follows: find settings of the discrete random vector \\boldsymbol{x} \\in \\mathbb{N}^{L} (e.g. representative of DNA sequences) that have a high probability of satisfying some property desiderata (e.g. maximal fluoresence of a protein). The inference problem is expressed as: p(\\boldsymbol{x}|S, \\theta^{(0)}) = \\frac{P(S|\\boldsymbol{x})p(\\boldsymbol{x}|\\theta^{(0)})}{P(S|\\theta^{(0)})} p(\\boldsymbol{x}|S, \\theta^{(0)}) = \\frac{P(S|\\boldsymbol{x})p(\\boldsymbol{x}|\\theta^{(0)})}{P(S|\\theta^{(0)})} where P(S|\\theta^{(0)}) = \\int_{s} P(S|\\boldsymbol{x})p(\\boldsymbol{x}|\\theta^{(0)}) d\\boldsymbol{x} . Like variational inference , the marginal probability of CbAS yields no analytic solution as the sum over all possible configurations of the latent structures and its parameters is intractable. Unsurprisingly, inference for CbAS is VI-like in that they minimize the KL divergence between the conditional and the approximating search model, q(\\boldsymbol{x}|\\phi) : \\phi^{*} = \\underset{\\phi}{\\operatorname{argmin}} D_{KL} \\big(p(\\boldsymbol{x}|S,\\theta^{(0)})||q(\\boldsymbol{x}|\\phi\\big) \\phi^{*} = \\underset{\\phi}{\\operatorname{argmin}} D_{KL} \\big(p(\\boldsymbol{x}|S,\\theta^{(0)})||q(\\boldsymbol{x}|\\phi\\big) This can be re-written in terms of the log expectation of the difference between the search model distribution and the target conditional distribution, similar to how the Evidence Lower Bound (ELBO) can be derived: \\underset{\\phi}{\\operatorname{argmin}}\\mathbb{E}_{p(\\boldsymbol{x}|S,\\theta^{(0)})}[\\log p(\\boldsymbol{x}|S,\\theta^{(0)}) - \\log q(\\boldsymbol{x}|\\phi)] \\underset{\\phi}{\\operatorname{argmin}}\\mathbb{E}_{p(\\boldsymbol{x}|S,\\theta^{(0)})}[\\log p(\\boldsymbol{x}|S,\\theta^{(0)}) - \\log q(\\boldsymbol{x}|\\phi)] Thus, the second expectation can be expressed as the entropy of the target conditional distribution H_{0} \\equiv -\\mathbb{E}_{p(\\boldsymbol{x}|S,\\theta^{(0)})}[\\log p(\\boldsymbol{x}|S,\\theta^{(0)})] : \\begin{align} & = \\underset{\\phi}{\\operatorname{argmin}}-\\mathbb{E}_{p(\\boldsymbol{x}|S,\\theta^{(0)})}[\\log q(\\boldsymbol{x}|\\phi)] - H_{0} \\newline & = \\underset{\\phi}{\\operatorname{argmax}} \\frac{1}{P(S|\\theta^{(0)})} \\mathbb{E}_{(p(\\boldsymbol{x}|S,\\theta^{(0)})}[P(S|\\boldsymbol{x}) \\log q(\\boldsymbol{x}|\\phi)] \\newline & = \\underset{\\phi}{\\operatorname{argmax}}\\mathbb{E}_{p(\\boldsymbol{x}|\\theta^{(0)})}[P(S|\\boldsymbol{x}) \\log q(\\boldsymbol{x}|\\phi)] & \\scriptstyle{H_{0}, P(S|\\theta^{(0)}) \\text{ dropped}} \\\\[2mm] \\end{align} The authors also note that the difficulty of estimating the normalizing constant is compounded by the fact that S (and by definition, P(S|\\boldsymbol{x}) ) is \"exceedingly rare\". This translates to difficulties in optimization: any gradient-based reparameterization which yields differentiable Monte Carlo (MC) estimates of our expectation (i.e. the 'reparameterization trick' from Kingma and Welling 2014 ) would require a non-trivial number of samples to be accurate and be very challenging for distributions over discrete state spaces. To put it another way, any naive MC estimation would require an intensive simulation effort, inversely proportional to the magnitude of P(S|\\boldsymbol{x}) .","title":"Conditioning by adaptive sampling"},{"location":"#importance-sampling-based-estimation","text":"I'm not going to exhaustively review importance sampling here 3 . Suffice to say that the basic idea is that we can estimate the expectation of a random variable under one distribution p(\\boldsymbol{x}) from samples of a different distribution r(\\boldsymbol{x}) and that our importance sampling distribution r(\\boldsymbol{x}) can be used to mitigate the problem of high variance and an exceedingly small expectation \\mathbb{E}_{r^{(t)}(\\boldsymbol{x})} \\big[P(S^{(t)}|\\boldsymbol{x}\\big] . This approach is not without drawbacks however, and perturbations in weight space can have deleterious effects on the estimator. Thus, the objective can be rewritten as: \\begin{align} & = \\mathbb{E}_{r(\\boldsymbol{x})}\\Bigg[\\frac{p(\\boldsymbol{x}|\\theta^{(0)})}{r(x)}[P(S|\\boldsymbol{x}) \\log q(\\boldsymbol{x}|\\phi)]\\Bigg] \\newline & = \\mathbb{E}_{q(\\boldsymbol{x}|\\phi^{(t)})}\\Bigg[\\frac{p(\\boldsymbol{x}|\\theta^{(0)})}{q(\\boldsymbol{x}|\\phi^{(t)})}[P(S^{(t)}|\\boldsymbol{x}) \\log q(\\boldsymbol{x}|\\phi)]\\Bigg] \\end{align} where p(\\boldsymbol{x}|\\theta^{(0)}) is our target density, q(\\boldsymbol{x}|\\phi^{(t)}) is our proposed density and the ratio of the two: \\frac{p(\\boldsymbol{x}|\\theta^{(0)})}{q(\\boldsymbol{x}|\\phi^{(t)})} \\frac{p(\\boldsymbol{x}|\\theta^{(0)})}{q(\\boldsymbol{x}|\\phi^{(t)})} is the eponymous importance weight . Now that we have this objective and given samples \\{x^{(i)} \\}^{(n)}_{i} from q(\\boldsymbol{x}|\\phi^{(t)}) , we can construct MC estimates: \\begin{align} & \\phi^{(t+1)} = \\underset{\\phi}{\\operatorname{argmin}}\\sum_{i=1}^{M} \\frac{p(\\boldsymbol{x}_{i}^{(t)}|\\theta^{(0)})}{q(\\boldsymbol{x}_{i}^{(t)}|\\phi^{(t)})} P(S^{(t)}|\\boldsymbol{x}_{i}^{(t)}) \\log q(\\boldsymbol{x}_{i}^{(t)}|\\phi)] \\end{align} I think this is a particularly clever reformulation because it does not constrain the authors' choice of search model distribution to a single model class. For example, for cases where marginalization over latent space is intractable and the CbAS objective cannot be solved exactly (i.e. where a vanilla VAE is used), approximate inference approaches can be used instead to extend the model class, something I alluded to earlier. I'm not going to go through the explicit derivation of the ELBO-like objective but it's worth noting that in many cases where the CbAS update can be solved, this product: \\begin{align} \\require{color} & \\phi^{(t+1)} = \\underset{\\phi}{\\operatorname{argmin}}\\sum_{i=1}^{M} \\colorbox{GreenYellow}{$\\frac{p(\\boldsymbol{x}_{i}^{(t)}|\\theta^{(0)})}{q(\\boldsymbol{x}_{i}^{(t)}|\\phi^{(t)})} P(S^{(t)}|\\boldsymbol{x}_{i}^{(t)})$} \\log q(\\boldsymbol{x}_{i}^{(t)}|\\phi)] \\end{align} translates to an importance weighting of the probability of a series of property value sets S^{(t)} conditioned on each data point, \\boldsymbol{x}_{i}^{(t)} (i.e. the weight of each data point). This is similar to a weighted maximum likelihood objective and can therefore be optimized using any number of standard approaches for training generative models. More to come on the experimental results in the next entry... A good entry point to Listgarten's earlier work is her interview on Talking Machines , where she discusses her work at MSR New England on predicting off-target effects for CRISPR guide design. \u21a9 Additional assumptions are that the oracle is well-behaved and noise-free \u21a9 For this, I recommend this chapter from Art Owen's book, \"Monte Carlo theory, methods and examples\". \u21a9","title":"Importance sampling-based estimation"},{"location":"03-26-19-revisiting-flows/","text":"Revisiting normalizing flows Fig 1: Samples from an unconditional model with affine coupling layers trained on the CIFAR-10 dataset with temperature 1.0 after 10 epochs ( left ) and 600 epochs ( right ) using Horovod (You can find our Dockerfile here ). If you recall from our previous discussion on Glow ( Kingma and Dhariwal 2018 ), an attractive aspect of normalizing flows is that the exact log-likelihood of input data \\log p(\\mathbf{x}) becomes tractable. As a result, the training criterion of a flow-based generative model is simply the negative log-likelihood (bits per dimension) over the training dataset \\mathcal{D} : \\mathcal{L}(\\mathcal{D}) = - \\frac{1}{\\vert\\mathcal{D}\\vert}\\sum_{\\mathbf{x} \\in \\mathcal{D}} \\log p(\\mathbf{x}) What distinguishes flow-based generative models' capacity for exact inference versus other types of generative models? Simply, it's that flow-based models combine several fundamental concepts as a substrate for transforming distributions in a way that optimization of the log-likelihood is tractable. I'll review the motivation and implementation of a normalizing flow through the use of bijective functions . Bijective functions In deep learning, rich high-dimensional densities typically use invertible volume-preserving mappings or mappings with fast volume adjustments -- i.e. the logarithm of the determinant of the Jacobian has linear complexity with respect to dimensionality. For a given probability distribution p_{Y}(\\mathbf{y}) , the density is: p_{Y}(\\mathbf{y}) = p_{X}(G^{-1}(\\mathbf{y}))\\ \\mathrm{det} ( \\frac{\\partial}{\\partial\\mathbf{y}} G^{-1}(\\mathbf{y} )) p_{Y}(\\mathbf{y}) = p_{X}(G^{-1}(\\mathbf{y}))\\ \\mathrm{det} ( \\frac{\\partial}{\\partial\\mathbf{y}} G^{-1}(\\mathbf{y} )) where G^{-1} is an inverse transformation and \\mathrm{det}(\\frac{\\partial}{\\partial\\mathbf{y}} G^{-1}(\\mathbf{y})) is the determinant of the Jacobian. We can then compute the log determinant of the Jacobian of the inverse transformation; we can also easily verify the \"volume-preserving\" property of the transformation: \\begin{align} \\mathrm{det} \\left ( \\frac{\\partial}{\\partial\\mathbf{y}} G^{-1}(\\mathbf{y}) \\right ) & = \\mathrm{det} \\begin{pmatrix} \\frac{\\partial}{\\partial y_1} y_1 & \\frac{\\partial}{\\partial y_2} y_1 \\newline \\frac{\\partial}{\\partial y_1} y_2 + y_1^2 + 1 & \\frac{\\partial}{\\partial y_2} y_2 + y_1^2 + 1 \\newline \\end{pmatrix} \\newline & = \\mathrm{det} \\begin{pmatrix} 1 & 0 \\newline 2 y_1 & 1 \\newline \\end{pmatrix} = 1 \\end{align} Fortunately for us, libraries like TensorFlow Probablility have already done some of the heavy lifting by providing an extensive selection of different probability distribution and bijector classes. Both the Distribution and Bijector classes are used to comprise a TransformedDistribution , which models p(y) given a base distribution (an instance of the Distribution class) and an invertible, differentiable transform Y = g(X) (an instance of the Bijector class), where g is a deterministic diffeomorphism . The TFP API declares three operations that characterize a Bijector : Forward Useful for turning one random outcome into another random outcome from a different distribution. Inverse Useful for \"reversing\" a transformation to compute one probability in terms of another. log_det_jacobian(x) \"The log of the absolute value of the determinant of the matrix of all first-order partial derivatives of the inverse function.\" Thus, creating our bijector by subclassing `Bijector``: import tensorflow_probability as tfp # forward transformation as instance method class Foo ( tfp . bijectors . Bijector ): def __init__ ( self , name = \"Foo\" ): super ( Foo , self ) . __init__ ( inverse_min_event_ndims = 1 , name = name ) def _forward ( self , x ): y_0 = x [ ... , 0 : 1 ] y_1 = x [ ... , 1 : 2 ] - y_0 ** 2 - 1 y_tail = x [ ... , 2 : - 1 ] return tf . concat ([ y_0 , y_1 , y_tail ], axis =- 1 ) Next, let's look at a simple example illustrating standard TransformedDistribution ops by constructing a Log-Normal distribution from a Normal distribution: ds = tf . contrib . distributions log_normal = ds . TransformedDistribution ( distribution = ds . Normal ( loc = 0. , scale = 1. ), bijector = ds . bijectors . Exp (), name = \"LogNormalTransformedDistribution\" ) We've instantiated a TranformedDistribution with an Exp bijector and Normal distribution with parameters loc = 0 and scale = 1 ( \\mu and \\Sigma , respectively). It's worth noting that in TFP, we can start to initialize batches of scalar-valued Normals (and other distributions) by broadcasting. Affine coupling layers As I mentioned, in the RealNVP (Real-valued Non-Volume Preserving; Dinh et al. 2017 ) paper normalizing flows are implemented by stacking a sequence of invertible bijective transformation functions. In each bijection f: \\mathbf{x} \\mapsto \\mathbf{y} , known as the affine coupling layer (ACL), the input dimensions are split into two parts: The first d dimensions remain the same. The second part, d+1 to D dimensions, undergo an affine transformation (\u201cscale-and-shift\u201d) and both the scale and shift parameters are functions of the first d dimensions. % <![CDATA[ \\begin{aligned} \\mathbf{y}_{1:d} &= \\mathbf{x}_{1:d} \\\\ \\mathbf{y}_{d+1:D} &= \\mathbf{x}_{d+1:D} \\odot \\exp({s(\\mathbf{x}_{1:d})}) + t(\\mathbf{x}_{1:d}) \\end{aligned} %]]> where s(.) and t(.) are scale and translation functions and both map \\mathbb{R}^d \\mapsto \\mathbb{R}^{D-d} . How would we go about coding this? Let's decompose this in pseudo-code. The first component of the input dimension is straightforward since it's simply: y [ 0 , d ] = x [ 0 : d ] However, the second component requires that we scale-and-shift the last D-d units contingent on the first d units only, while the first d units are masked and left unchanged. In TensorFlow, this would look like: y [ d : D ] = x [ d : D ] * tf . exp ( log_scale_fn ( x [ 0 : d ])) + shift_fn ( x [ 0 : d ])","title":"Revisiting normalizing flows"},{"location":"03-26-19-revisiting-flows/#revisiting-normalizing-flows","text":"Fig 1: Samples from an unconditional model with affine coupling layers trained on the CIFAR-10 dataset with temperature 1.0 after 10 epochs ( left ) and 600 epochs ( right ) using Horovod (You can find our Dockerfile here ). If you recall from our previous discussion on Glow ( Kingma and Dhariwal 2018 ), an attractive aspect of normalizing flows is that the exact log-likelihood of input data \\log p(\\mathbf{x}) becomes tractable. As a result, the training criterion of a flow-based generative model is simply the negative log-likelihood (bits per dimension) over the training dataset \\mathcal{D} : \\mathcal{L}(\\mathcal{D}) = - \\frac{1}{\\vert\\mathcal{D}\\vert}\\sum_{\\mathbf{x} \\in \\mathcal{D}} \\log p(\\mathbf{x}) What distinguishes flow-based generative models' capacity for exact inference versus other types of generative models? Simply, it's that flow-based models combine several fundamental concepts as a substrate for transforming distributions in a way that optimization of the log-likelihood is tractable. I'll review the motivation and implementation of a normalizing flow through the use of bijective functions .","title":"Revisiting normalizing flows"},{"location":"03-26-19-revisiting-flows/#bijective-functions","text":"In deep learning, rich high-dimensional densities typically use invertible volume-preserving mappings or mappings with fast volume adjustments -- i.e. the logarithm of the determinant of the Jacobian has linear complexity with respect to dimensionality. For a given probability distribution p_{Y}(\\mathbf{y}) , the density is: p_{Y}(\\mathbf{y}) = p_{X}(G^{-1}(\\mathbf{y}))\\ \\mathrm{det} ( \\frac{\\partial}{\\partial\\mathbf{y}} G^{-1}(\\mathbf{y} )) p_{Y}(\\mathbf{y}) = p_{X}(G^{-1}(\\mathbf{y}))\\ \\mathrm{det} ( \\frac{\\partial}{\\partial\\mathbf{y}} G^{-1}(\\mathbf{y} )) where G^{-1} is an inverse transformation and \\mathrm{det}(\\frac{\\partial}{\\partial\\mathbf{y}} G^{-1}(\\mathbf{y})) is the determinant of the Jacobian. We can then compute the log determinant of the Jacobian of the inverse transformation; we can also easily verify the \"volume-preserving\" property of the transformation: \\begin{align} \\mathrm{det} \\left ( \\frac{\\partial}{\\partial\\mathbf{y}} G^{-1}(\\mathbf{y}) \\right ) & = \\mathrm{det} \\begin{pmatrix} \\frac{\\partial}{\\partial y_1} y_1 & \\frac{\\partial}{\\partial y_2} y_1 \\newline \\frac{\\partial}{\\partial y_1} y_2 + y_1^2 + 1 & \\frac{\\partial}{\\partial y_2} y_2 + y_1^2 + 1 \\newline \\end{pmatrix} \\newline & = \\mathrm{det} \\begin{pmatrix} 1 & 0 \\newline 2 y_1 & 1 \\newline \\end{pmatrix} = 1 \\end{align} Fortunately for us, libraries like TensorFlow Probablility have already done some of the heavy lifting by providing an extensive selection of different probability distribution and bijector classes. Both the Distribution and Bijector classes are used to comprise a TransformedDistribution , which models p(y) given a base distribution (an instance of the Distribution class) and an invertible, differentiable transform Y = g(X) (an instance of the Bijector class), where g is a deterministic diffeomorphism . The TFP API declares three operations that characterize a Bijector : Forward Useful for turning one random outcome into another random outcome from a different distribution. Inverse Useful for \"reversing\" a transformation to compute one probability in terms of another. log_det_jacobian(x) \"The log of the absolute value of the determinant of the matrix of all first-order partial derivatives of the inverse function.\" Thus, creating our bijector by subclassing `Bijector``: import tensorflow_probability as tfp # forward transformation as instance method class Foo ( tfp . bijectors . Bijector ): def __init__ ( self , name = \"Foo\" ): super ( Foo , self ) . __init__ ( inverse_min_event_ndims = 1 , name = name ) def _forward ( self , x ): y_0 = x [ ... , 0 : 1 ] y_1 = x [ ... , 1 : 2 ] - y_0 ** 2 - 1 y_tail = x [ ... , 2 : - 1 ] return tf . concat ([ y_0 , y_1 , y_tail ], axis =- 1 ) Next, let's look at a simple example illustrating standard TransformedDistribution ops by constructing a Log-Normal distribution from a Normal distribution: ds = tf . contrib . distributions log_normal = ds . TransformedDistribution ( distribution = ds . Normal ( loc = 0. , scale = 1. ), bijector = ds . bijectors . Exp (), name = \"LogNormalTransformedDistribution\" ) We've instantiated a TranformedDistribution with an Exp bijector and Normal distribution with parameters loc = 0 and scale = 1 ( \\mu and \\Sigma , respectively). It's worth noting that in TFP, we can start to initialize batches of scalar-valued Normals (and other distributions) by broadcasting.","title":"Bijective functions"},{"location":"03-26-19-revisiting-flows/#affine-coupling-layers","text":"As I mentioned, in the RealNVP (Real-valued Non-Volume Preserving; Dinh et al. 2017 ) paper normalizing flows are implemented by stacking a sequence of invertible bijective transformation functions. In each bijection f: \\mathbf{x} \\mapsto \\mathbf{y} , known as the affine coupling layer (ACL), the input dimensions are split into two parts: The first d dimensions remain the same. The second part, d+1 to D dimensions, undergo an affine transformation (\u201cscale-and-shift\u201d) and both the scale and shift parameters are functions of the first d dimensions. % <![CDATA[ \\begin{aligned} \\mathbf{y}_{1:d} &= \\mathbf{x}_{1:d} \\\\ \\mathbf{y}_{d+1:D} &= \\mathbf{x}_{d+1:D} \\odot \\exp({s(\\mathbf{x}_{1:d})}) + t(\\mathbf{x}_{1:d}) \\end{aligned} %]]> where s(.) and t(.) are scale and translation functions and both map \\mathbb{R}^d \\mapsto \\mathbb{R}^{D-d} . How would we go about coding this? Let's decompose this in pseudo-code. The first component of the input dimension is straightforward since it's simply: y [ 0 , d ] = x [ 0 : d ] However, the second component requires that we scale-and-shift the last D-d units contingent on the first d units only, while the first d units are masked and left unchanged. In TensorFlow, this would look like: y [ d : D ] = x [ d : D ] * tf . exp ( log_scale_fn ( x [ 0 : d ])) + shift_fn ( x [ 0 : d ])","title":"Affine coupling layers"},{"location":"05-07-19-adaptive-sampling/","text":"On conditioning by adaptive sampling (Part I) A recent paper by David Brookes, Hahnbeom Park, and Jennifer Listgarten 1 extends their work from last year that posits a framework, Conditioning by Adaptive Sampling (CbAS) , for model-based conditional density estimation. Of particular relevance to us here at Pfizer is the application to protein design. For example, how do we go about optimizing the design of DNA sequences for protein expression, binding, or for protein sequences that maximize properties like secondary structure or fluorescence? Traditionally, in vitro methods for protein engineering have been exhaustive usually involving greedy walks through protein design space like directed evolution (DE) . This paper proposes a more efficient in silico approach. For this particular entry, I'll summarize the key contributions and introduce some of the core concepts. The subsequent post will walk through the Cross-Entropy Method (CEM) and their experimental results. Summary The main contributions of this paper are as follows: The authors present a method for tackling design problems in discrete space including the problem of maximizing protein fluorescence using avGFP sequence data from Sarkisyan et al. 2016 . A principled approach for conditional density estimation in the presence of rare events and which doesn't constrain the oracle by requiring differentiability. This has the added benefit of recapitulating models in various scientific domains which may not be readily differentiable to begin with It also allows for a degree of flexibility of validation by using wet-lab experiments as the oracle. An exact and iterative importance sampling-based estimation method for variance reduction and computationally intractable model densities. Generalizability of CbAS to specification of property values rather than solely maximization (e.g. a protein fluorescing at a desired wavelength as opposed to being maximally fluorescent). Oracles and oracle properties In order to understand the problem setting in the paper, it's worth briefly reviewing oracles. In statistic learning, oracle and oracle properties have been extensively used in the context of high-dimensional model estimation and feature selection whereby we have a response vector \\boldsymbol{y} = (y_1,...,y_n)^{T} and input vector of linearly independent features \\boldsymbol{x_j} = (x_{1j},...,x_{nj})^{T} with j = 1,..,p . Under the assumption that we have \\mathbb{A} = \\{ j: \\hat{\\beta} \\neq 0 \\} , the true model only depends on a subset X_{S} \\subset X of its feature space. Briefly put, an oracle property is a theoretical attribute ascribed to a model if it performs as well as if the correct underlying model were given in advance. More formally and following Fan and Li 2001 , Fan and Peng 2004 , and Zou 2006 , we call a given fitting procedure \\delta an oracle if \\hat{\\beta}(\\delta) , the coefficient estimator, identifies the right subset model, \\{ j: \\hat{\\beta} \\neq 0 \\} = \\mathbb{A} with some optimal estimation rate: \\sqrt{n} \\hat{\\beta}(\\delta)_{\\mathbb{A}} - \\beta^*_{\\mathbb{A}}) \\to \\mathcal{N} (0, I^{-1}(\\beta^*_\\mathbb{A})) Thus, the oracle property holds that the asymptotic distribution of the estimator is the same as the asymptotic distribution of the MLE only on the true support. That is, the estimator adapts to knowing the true support without incurring a price. Why is this important? For the authors, they argue that their oracle p(y|\\boldsymbol{x}) effectively serves as a proxy for expensive and exhaustive laboratory-based property measurements. They also make the assumption that given an input, such as a DNA sequence, the oracle 2 returns a distribution over the properties of interest. It's worth noting that this is within the context of the generative setting where we model the joint probability distribution p(\\boldsymbol{x}, y) . Given the property oracle then, one can subsequently model and empirically sample from the underlying data distribution p_{d}(\\boldsymbol{x}) with a generative model p(\\boldsymbol{x}|\\theta) whose prior is conditioned on the desiderata encoded in property values S . The claim here is that this gives a certain degree of flexibility in specifying priors on the design space, unlike G\u00f3mez-Bombarelli et al. 2018 and Killoran et al. 2017 , which utilize a fixed VAE and GAN respectively. Lastly, it's worth noting that they do not require their oracle to be differentiable and this additionally circumvents backpropagation through discrete input space. Conditioning by adaptive sampling The problem setting is as follows: find settings of the discrete random vector \\boldsymbol{x} \\in \\mathbb{N}^{L} (e.g. representative of DNA sequences) that have a high probability of satisfying some property desiderata (e.g. maximal fluoresence of a protein). The inference problem is expressed as: p(\\boldsymbol{x}|S, \\theta^{(0)}) = \\frac{P(S|\\boldsymbol{x})p(\\boldsymbol{x}|\\theta^{(0)})}{P(S|\\theta^{(0)})} p(\\boldsymbol{x}|S, \\theta^{(0)}) = \\frac{P(S|\\boldsymbol{x})p(\\boldsymbol{x}|\\theta^{(0)})}{P(S|\\theta^{(0)})} where P(S|\\theta^{(0)}) = \\int_{s} P(S|\\boldsymbol{x})p(\\boldsymbol{x}|\\theta^{(0)}) d\\boldsymbol{x} . Like variational inference , the marginal probability of CbAS yields no analytic solution as the sum over all possible configurations of the latent structures and its parameters is intractable. Unsurprisingly, inference for CbAS is VI-like in that they minimize the KL divergence between the conditional and the approximating search model, q(\\boldsymbol{x}|\\phi) : \\phi^{*} = \\underset{\\phi}{\\operatorname{argmin}} D_{KL} \\big(p(\\boldsymbol{x}|S,\\theta^{(0)})||q(\\boldsymbol{x}|\\phi\\big) \\phi^{*} = \\underset{\\phi}{\\operatorname{argmin}} D_{KL} \\big(p(\\boldsymbol{x}|S,\\theta^{(0)})||q(\\boldsymbol{x}|\\phi\\big) This can be re-written in terms of the log expectation of the difference between the search model distribution and the target conditional distribution, similar to how the Evidence Lower Bound (ELBO) can be derived: \\underset{\\phi}{\\operatorname{argmin}}\\mathbb{E}_{p(\\boldsymbol{x}|S,\\theta^{(0)})}[\\log p(\\boldsymbol{x}|S,\\theta^{(0)}) - \\log q(\\boldsymbol{x}|\\phi)] \\underset{\\phi}{\\operatorname{argmin}}\\mathbb{E}_{p(\\boldsymbol{x}|S,\\theta^{(0)})}[\\log p(\\boldsymbol{x}|S,\\theta^{(0)}) - \\log q(\\boldsymbol{x}|\\phi)] Thus, the second expectation can be expressed as the entropy of the target conditional distribution H_{0} \\equiv -\\mathbb{E}_{p(\\boldsymbol{x}|S,\\theta^{(0)})}[\\log p(\\boldsymbol{x}|S,\\theta^{(0)})] : \\begin{align} & = \\underset{\\phi}{\\operatorname{argmin}}-\\mathbb{E}_{p(\\boldsymbol{x}|S,\\theta^{(0)})}[\\log q(\\boldsymbol{x}|\\phi)] - H_{0} \\newline & = \\underset{\\phi}{\\operatorname{argmax}} \\frac{1}{P(S|\\theta^{(0)})} \\mathbb{E}_{(p(\\boldsymbol{x}|S,\\theta^{(0)})}[P(S|\\boldsymbol{x}) \\log q(\\boldsymbol{x}|\\phi)] \\newline & = \\underset{\\phi}{\\operatorname{argmax}}\\mathbb{E}_{p(\\boldsymbol{x}|\\theta^{(0)})}[P(S|\\boldsymbol{x}) \\log q(\\boldsymbol{x}|\\phi)] & \\scriptstyle{H_{0}, P(S|\\theta^{(0)}) \\text{ dropped}} \\\\[2mm] \\end{align} The authors also note that the difficulty of estimating the normalizing constant is compounded by the fact that S (and by definition, P(S|\\boldsymbol{x}) ) is \"exceedingly rare\". This translates to difficulties in optimization: any gradient-based reparameterization which yields differentiable Monte Carlo (MC) estimates of our expectation (i.e. the 'reparameterization trick' from Kingma and Welling 2014 ) would require a non-trivial number of samples to be accurate and be very challenging for distributions over discrete state spaces. To put it another way, any naive MC estimation would require an intensive simulation effort, inversely proportional to the magnitude of P(S|\\boldsymbol{x}) . Importance sampling-based estimation I'm not going to exhaustively review importance sampling here 3 . Suffice to say that the basic idea is that we can estimate the expectation of a random variable under one distribution p(\\boldsymbol{x}) from samples of a different distribution r(\\boldsymbol{x}) and that our importance sampling distribution r(\\boldsymbol{x}) can be used to mitigate the problem of high variance and an exceedingly small expectation \\mathbb{E}_{r^{(t)}(\\boldsymbol{x})} \\big[P(S^{(t)}|\\boldsymbol{x}\\big] . This approach is not without drawbacks however, and perturbations in weight space can have deleterious effects on the estimator. Thus, the objective can be rewritten as: \\begin{align} & = \\mathbb{E}_{r(\\boldsymbol{x})}\\Bigg[\\frac{p(\\boldsymbol{x}|\\theta^{(0)})}{r(x)}[P(S|\\boldsymbol{x}) \\log q(\\boldsymbol{x}|\\phi)]\\Bigg] \\newline & = \\mathbb{E}_{q(\\boldsymbol{x}|\\phi^{(t)})}\\Bigg[\\frac{p(\\boldsymbol{x}|\\theta^{(0)})}{q(\\boldsymbol{x}|\\phi^{(t)})}[P(S^{(t)}|\\boldsymbol{x}) \\log q(\\boldsymbol{x}|\\phi)]\\Bigg] \\end{align} where p(\\boldsymbol{x}|\\theta^{(0)}) is our target density, q(\\boldsymbol{x}|\\phi^{(t)}) is our proposed density and the ratio of the two: \\frac{p(\\boldsymbol{x}|\\theta^{(0)})}{q(\\boldsymbol{x}|\\phi^{(t)})} \\frac{p(\\boldsymbol{x}|\\theta^{(0)})}{q(\\boldsymbol{x}|\\phi^{(t)})} is the eponymous importance weight . Now that we have this objective and given samples \\{x^{(i)} \\}^{(n)}_{i} from q(\\boldsymbol{x}|\\phi^{(t)}) , we can construct MC estimates: \\begin{align} & \\phi^{(t+1)} = \\underset{\\phi}{\\operatorname{argmin}}\\sum_{i=1}^{M} \\frac{p(\\boldsymbol{x}_{i}^{(t)}|\\theta^{(0)})}{q(\\boldsymbol{x}_{i}^{(t)}|\\phi^{(t)})} P(S^{(t)}|\\boldsymbol{x}_{i}^{(t)}) \\log q(\\boldsymbol{x}_{i}^{(t)}|\\phi)] \\end{align} I think this is a particularly clever reformulation because it does not constrain the authors' choice of search model distribution to a single model class. For example, for cases where marginalization over latent space is intractable and the CbAS objective cannot be solved exactly (i.e. where a vanilla VAE is used), approximate inference approaches can be used instead to extend the model class, something I alluded to earlier. I'm not going to go through the explicit derivation of the ELBO-like objective but it's worth noting that in many cases where the CbAS update can be solved, this product: \\begin{align} \\require{color} & \\phi^{(t+1)} = \\underset{\\phi}{\\operatorname{argmin}}\\sum_{i=1}^{M} \\colorbox{GreenYellow}{$\\frac{p(\\boldsymbol{x}_{i}^{(t)}|\\theta^{(0)})}{q(\\boldsymbol{x}_{i}^{(t)}|\\phi^{(t)})} P(S^{(t)}|\\boldsymbol{x}_{i}^{(t)})$} \\log q(\\boldsymbol{x}_{i}^{(t)}|\\phi)] \\end{align} translates to an importance weighting of the probability of a series of property value sets S^{(t)} conditioned on each data point, \\boldsymbol{x}_{i}^{(t)} (i.e. the weight of each data point). This is similar to a weighted maximum likelihood objective and can therefore be optimized using any number of standard approaches for training generative models. More to come on the experimental results in the next entry... A good entry point to Listgarten's earlier work is her interview on Talking Machines , where she discusses her work at MSR New England on predicting off-target effects for CRISPR guide design. \u21a9 Additional assumptions are that the oracle is well-behaved and noise-free \u21a9 For this, I recommend this chapter from Art Owen's book, \"Monte Carlo theory, methods and examples\". \u21a9","title":"On conditioning by adaptive sampling"},{"location":"05-07-19-adaptive-sampling/#on-conditioning-by-adaptive-sampling-part-i","text":"A recent paper by David Brookes, Hahnbeom Park, and Jennifer Listgarten 1 extends their work from last year that posits a framework, Conditioning by Adaptive Sampling (CbAS) , for model-based conditional density estimation. Of particular relevance to us here at Pfizer is the application to protein design. For example, how do we go about optimizing the design of DNA sequences for protein expression, binding, or for protein sequences that maximize properties like secondary structure or fluorescence? Traditionally, in vitro methods for protein engineering have been exhaustive usually involving greedy walks through protein design space like directed evolution (DE) . This paper proposes a more efficient in silico approach. For this particular entry, I'll summarize the key contributions and introduce some of the core concepts. The subsequent post will walk through the Cross-Entropy Method (CEM) and their experimental results.","title":"On conditioning by adaptive sampling (Part I)"},{"location":"05-07-19-adaptive-sampling/#summary","text":"The main contributions of this paper are as follows: The authors present a method for tackling design problems in discrete space including the problem of maximizing protein fluorescence using avGFP sequence data from Sarkisyan et al. 2016 . A principled approach for conditional density estimation in the presence of rare events and which doesn't constrain the oracle by requiring differentiability. This has the added benefit of recapitulating models in various scientific domains which may not be readily differentiable to begin with It also allows for a degree of flexibility of validation by using wet-lab experiments as the oracle. An exact and iterative importance sampling-based estimation method for variance reduction and computationally intractable model densities. Generalizability of CbAS to specification of property values rather than solely maximization (e.g. a protein fluorescing at a desired wavelength as opposed to being maximally fluorescent).","title":"Summary"},{"location":"05-07-19-adaptive-sampling/#oracles-and-oracle-properties","text":"In order to understand the problem setting in the paper, it's worth briefly reviewing oracles. In statistic learning, oracle and oracle properties have been extensively used in the context of high-dimensional model estimation and feature selection whereby we have a response vector \\boldsymbol{y} = (y_1,...,y_n)^{T} and input vector of linearly independent features \\boldsymbol{x_j} = (x_{1j},...,x_{nj})^{T} with j = 1,..,p . Under the assumption that we have \\mathbb{A} = \\{ j: \\hat{\\beta} \\neq 0 \\} , the true model only depends on a subset X_{S} \\subset X of its feature space. Briefly put, an oracle property is a theoretical attribute ascribed to a model if it performs as well as if the correct underlying model were given in advance. More formally and following Fan and Li 2001 , Fan and Peng 2004 , and Zou 2006 , we call a given fitting procedure \\delta an oracle if \\hat{\\beta}(\\delta) , the coefficient estimator, identifies the right subset model, \\{ j: \\hat{\\beta} \\neq 0 \\} = \\mathbb{A} with some optimal estimation rate: \\sqrt{n} \\hat{\\beta}(\\delta)_{\\mathbb{A}} - \\beta^*_{\\mathbb{A}}) \\to \\mathcal{N} (0, I^{-1}(\\beta^*_\\mathbb{A})) Thus, the oracle property holds that the asymptotic distribution of the estimator is the same as the asymptotic distribution of the MLE only on the true support. That is, the estimator adapts to knowing the true support without incurring a price. Why is this important? For the authors, they argue that their oracle p(y|\\boldsymbol{x}) effectively serves as a proxy for expensive and exhaustive laboratory-based property measurements. They also make the assumption that given an input, such as a DNA sequence, the oracle 2 returns a distribution over the properties of interest. It's worth noting that this is within the context of the generative setting where we model the joint probability distribution p(\\boldsymbol{x}, y) . Given the property oracle then, one can subsequently model and empirically sample from the underlying data distribution p_{d}(\\boldsymbol{x}) with a generative model p(\\boldsymbol{x}|\\theta) whose prior is conditioned on the desiderata encoded in property values S . The claim here is that this gives a certain degree of flexibility in specifying priors on the design space, unlike G\u00f3mez-Bombarelli et al. 2018 and Killoran et al. 2017 , which utilize a fixed VAE and GAN respectively. Lastly, it's worth noting that they do not require their oracle to be differentiable and this additionally circumvents backpropagation through discrete input space.","title":"Oracles and oracle properties"},{"location":"05-07-19-adaptive-sampling/#conditioning-by-adaptive-sampling","text":"The problem setting is as follows: find settings of the discrete random vector \\boldsymbol{x} \\in \\mathbb{N}^{L} (e.g. representative of DNA sequences) that have a high probability of satisfying some property desiderata (e.g. maximal fluoresence of a protein). The inference problem is expressed as: p(\\boldsymbol{x}|S, \\theta^{(0)}) = \\frac{P(S|\\boldsymbol{x})p(\\boldsymbol{x}|\\theta^{(0)})}{P(S|\\theta^{(0)})} p(\\boldsymbol{x}|S, \\theta^{(0)}) = \\frac{P(S|\\boldsymbol{x})p(\\boldsymbol{x}|\\theta^{(0)})}{P(S|\\theta^{(0)})} where P(S|\\theta^{(0)}) = \\int_{s} P(S|\\boldsymbol{x})p(\\boldsymbol{x}|\\theta^{(0)}) d\\boldsymbol{x} . Like variational inference , the marginal probability of CbAS yields no analytic solution as the sum over all possible configurations of the latent structures and its parameters is intractable. Unsurprisingly, inference for CbAS is VI-like in that they minimize the KL divergence between the conditional and the approximating search model, q(\\boldsymbol{x}|\\phi) : \\phi^{*} = \\underset{\\phi}{\\operatorname{argmin}} D_{KL} \\big(p(\\boldsymbol{x}|S,\\theta^{(0)})||q(\\boldsymbol{x}|\\phi\\big) \\phi^{*} = \\underset{\\phi}{\\operatorname{argmin}} D_{KL} \\big(p(\\boldsymbol{x}|S,\\theta^{(0)})||q(\\boldsymbol{x}|\\phi\\big) This can be re-written in terms of the log expectation of the difference between the search model distribution and the target conditional distribution, similar to how the Evidence Lower Bound (ELBO) can be derived: \\underset{\\phi}{\\operatorname{argmin}}\\mathbb{E}_{p(\\boldsymbol{x}|S,\\theta^{(0)})}[\\log p(\\boldsymbol{x}|S,\\theta^{(0)}) - \\log q(\\boldsymbol{x}|\\phi)] \\underset{\\phi}{\\operatorname{argmin}}\\mathbb{E}_{p(\\boldsymbol{x}|S,\\theta^{(0)})}[\\log p(\\boldsymbol{x}|S,\\theta^{(0)}) - \\log q(\\boldsymbol{x}|\\phi)] Thus, the second expectation can be expressed as the entropy of the target conditional distribution H_{0} \\equiv -\\mathbb{E}_{p(\\boldsymbol{x}|S,\\theta^{(0)})}[\\log p(\\boldsymbol{x}|S,\\theta^{(0)})] : \\begin{align} & = \\underset{\\phi}{\\operatorname{argmin}}-\\mathbb{E}_{p(\\boldsymbol{x}|S,\\theta^{(0)})}[\\log q(\\boldsymbol{x}|\\phi)] - H_{0} \\newline & = \\underset{\\phi}{\\operatorname{argmax}} \\frac{1}{P(S|\\theta^{(0)})} \\mathbb{E}_{(p(\\boldsymbol{x}|S,\\theta^{(0)})}[P(S|\\boldsymbol{x}) \\log q(\\boldsymbol{x}|\\phi)] \\newline & = \\underset{\\phi}{\\operatorname{argmax}}\\mathbb{E}_{p(\\boldsymbol{x}|\\theta^{(0)})}[P(S|\\boldsymbol{x}) \\log q(\\boldsymbol{x}|\\phi)] & \\scriptstyle{H_{0}, P(S|\\theta^{(0)}) \\text{ dropped}} \\\\[2mm] \\end{align} The authors also note that the difficulty of estimating the normalizing constant is compounded by the fact that S (and by definition, P(S|\\boldsymbol{x}) ) is \"exceedingly rare\". This translates to difficulties in optimization: any gradient-based reparameterization which yields differentiable Monte Carlo (MC) estimates of our expectation (i.e. the 'reparameterization trick' from Kingma and Welling 2014 ) would require a non-trivial number of samples to be accurate and be very challenging for distributions over discrete state spaces. To put it another way, any naive MC estimation would require an intensive simulation effort, inversely proportional to the magnitude of P(S|\\boldsymbol{x}) .","title":"Conditioning by adaptive sampling"},{"location":"05-07-19-adaptive-sampling/#importance-sampling-based-estimation","text":"I'm not going to exhaustively review importance sampling here 3 . Suffice to say that the basic idea is that we can estimate the expectation of a random variable under one distribution p(\\boldsymbol{x}) from samples of a different distribution r(\\boldsymbol{x}) and that our importance sampling distribution r(\\boldsymbol{x}) can be used to mitigate the problem of high variance and an exceedingly small expectation \\mathbb{E}_{r^{(t)}(\\boldsymbol{x})} \\big[P(S^{(t)}|\\boldsymbol{x}\\big] . This approach is not without drawbacks however, and perturbations in weight space can have deleterious effects on the estimator. Thus, the objective can be rewritten as: \\begin{align} & = \\mathbb{E}_{r(\\boldsymbol{x})}\\Bigg[\\frac{p(\\boldsymbol{x}|\\theta^{(0)})}{r(x)}[P(S|\\boldsymbol{x}) \\log q(\\boldsymbol{x}|\\phi)]\\Bigg] \\newline & = \\mathbb{E}_{q(\\boldsymbol{x}|\\phi^{(t)})}\\Bigg[\\frac{p(\\boldsymbol{x}|\\theta^{(0)})}{q(\\boldsymbol{x}|\\phi^{(t)})}[P(S^{(t)}|\\boldsymbol{x}) \\log q(\\boldsymbol{x}|\\phi)]\\Bigg] \\end{align} where p(\\boldsymbol{x}|\\theta^{(0)}) is our target density, q(\\boldsymbol{x}|\\phi^{(t)}) is our proposed density and the ratio of the two: \\frac{p(\\boldsymbol{x}|\\theta^{(0)})}{q(\\boldsymbol{x}|\\phi^{(t)})} \\frac{p(\\boldsymbol{x}|\\theta^{(0)})}{q(\\boldsymbol{x}|\\phi^{(t)})} is the eponymous importance weight . Now that we have this objective and given samples \\{x^{(i)} \\}^{(n)}_{i} from q(\\boldsymbol{x}|\\phi^{(t)}) , we can construct MC estimates: \\begin{align} & \\phi^{(t+1)} = \\underset{\\phi}{\\operatorname{argmin}}\\sum_{i=1}^{M} \\frac{p(\\boldsymbol{x}_{i}^{(t)}|\\theta^{(0)})}{q(\\boldsymbol{x}_{i}^{(t)}|\\phi^{(t)})} P(S^{(t)}|\\boldsymbol{x}_{i}^{(t)}) \\log q(\\boldsymbol{x}_{i}^{(t)}|\\phi)] \\end{align} I think this is a particularly clever reformulation because it does not constrain the authors' choice of search model distribution to a single model class. For example, for cases where marginalization over latent space is intractable and the CbAS objective cannot be solved exactly (i.e. where a vanilla VAE is used), approximate inference approaches can be used instead to extend the model class, something I alluded to earlier. I'm not going to go through the explicit derivation of the ELBO-like objective but it's worth noting that in many cases where the CbAS update can be solved, this product: \\begin{align} \\require{color} & \\phi^{(t+1)} = \\underset{\\phi}{\\operatorname{argmin}}\\sum_{i=1}^{M} \\colorbox{GreenYellow}{$\\frac{p(\\boldsymbol{x}_{i}^{(t)}|\\theta^{(0)})}{q(\\boldsymbol{x}_{i}^{(t)}|\\phi^{(t)})} P(S^{(t)}|\\boldsymbol{x}_{i}^{(t)})$} \\log q(\\boldsymbol{x}_{i}^{(t)}|\\phi)] \\end{align} translates to an importance weighting of the probability of a series of property value sets S^{(t)} conditioned on each data point, \\boldsymbol{x}_{i}^{(t)} (i.e. the weight of each data point). This is similar to a weighted maximum likelihood objective and can therefore be optimized using any number of standard approaches for training generative models. More to come on the experimental results in the next entry... A good entry point to Listgarten's earlier work is her interview on Talking Machines , where she discusses her work at MSR New England on predicting off-target effects for CRISPR guide design. \u21a9 Additional assumptions are that the oracle is well-behaved and noise-free \u21a9 For this, I recommend this chapter from Art Owen's book, \"Monte Carlo theory, methods and examples\". \u21a9","title":"Importance sampling-based estimation"},{"location":"05-14-19-adaptive-sampling-ii/","text":"Importance sampling-based estimation I'm not going to review importance sampling here[^2], suffice to say that an importance sampling distribution r(\\boldsymbol{x}) can be used to mitigate the problem of high variance and an exceedingly small expectation \\mathbb{E}_{r^{(t)}(\\boldsymbol{x})} \\big[P(S^{(t)}|\\boldsymbol{x}\\big] . Thus, the objective can be rewritten as: \\mathbb{E}_{r(\\boldsymbol{x})}\\Bigg[\\frac{p(\\boldsymbol{x}|\\theta^{(0)})}{r(x)}[P(S|\\boldsymbol{x}) \\log q(\\boldsymbol{x}|\\phi)]\\Bigg] \\mathbb{E}_{r(\\boldsymbol{x})}\\Bigg[\\frac{p(\\boldsymbol{x}|\\theta^{(0)})}{r(x)}[P(S|\\boldsymbol{x}) \\log q(\\boldsymbol{x}|\\phi)]\\Bigg] Bears similarity with the Cross-Entropy Method (CEM) of Rubinstein 1999 . The main idea with CEM is to transform a given optimization problem to an associated stochastic problem (ASP)","title":"05 14 19 adaptive sampling ii"},{"location":"05-14-19-adaptive-sampling-ii/#importance-sampling-based-estimation","text":"I'm not going to review importance sampling here[^2], suffice to say that an importance sampling distribution r(\\boldsymbol{x}) can be used to mitigate the problem of high variance and an exceedingly small expectation \\mathbb{E}_{r^{(t)}(\\boldsymbol{x})} \\big[P(S^{(t)}|\\boldsymbol{x}\\big] . Thus, the objective can be rewritten as: \\mathbb{E}_{r(\\boldsymbol{x})}\\Bigg[\\frac{p(\\boldsymbol{x}|\\theta^{(0)})}{r(x)}[P(S|\\boldsymbol{x}) \\log q(\\boldsymbol{x}|\\phi)]\\Bigg] \\mathbb{E}_{r(\\boldsymbol{x})}\\Bigg[\\frac{p(\\boldsymbol{x}|\\theta^{(0)})}{r(x)}[P(S|\\boldsymbol{x}) \\log q(\\boldsymbol{x}|\\phi)]\\Bigg] Bears similarity with the Cross-Entropy Method (CEM) of Rubinstein 1999 . The main idea with CEM is to transform a given optimization problem to an associated stochastic problem (ASP)","title":"Importance sampling-based estimation"},{"location":"about/","text":"The Machine Learning group is a research team within Simulation and Modeling Sciences . As part of R&D, the team focuses on developing theory and methods in machine learning with the goal of solving difficult, yet tractable problems in drug discovery. We have interests at the intersection of computational sciences, Bayesian statistics, and machine learning.","title":"Mission"},{"location":"de-novo/","text":"","title":"De novo molecule generation"},{"location":"dgms/","text":"","title":"Dgms"},{"location":"publications/","text":"Deep learning of representations for transcriptomics-based phenotype prediction Aaron M. Smith, Jonathan R. Walsh, John Long, Craig B. Davis, Peter Henstock, Martin R. Hodge, Mateusz Maciejewski, Xinmeng Jasminue Mu, Stephen Ra, Shanrong Zhao, Daniel Ziemek, Charles K. Fisher bioRxiv , 2019. doi:10.1101/574723 Paper Deep generative models for the directed expansion of compound libraries Vishnu Sresht, Stephen Ra, Brajesh Rai, Bruce A. Lefker, Alan Mathiowetz American Chemical Society (ACS) National Meeting 2018 , 2012; 17, 1054-55. doi:10.1038/mp.2012.71 Paper","title":"Publications"},{"location":"team/","text":"","title":"Team"}]}