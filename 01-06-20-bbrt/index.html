
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
        <meta name="description" content="Personal website of Stephen Ra">
      
      
      
        <meta name="author" content="Stephen Ra">
      
      
        <link rel="canonical" href="https://www.stephenra.com/01-06-20-bbrt/">
      
      <link rel="shortcut icon" href="../img/favicon.png">
      <meta name="generator" content="mkdocs-1.1.2, mkdocs-material-7.0.3">
    
    
      
        <title>Black box recursive translations for molecular optimization - Stephen Ra</title>
      
    
    
      <link rel="stylesheet" href="../assets/stylesheets/main.1655a90d.min.css">
      
        
        <link rel="stylesheet" href="../assets/stylesheets/palette.7fa14f5b.min.css">
        
          
          
          <meta name="theme-color" content="#ffffff">
        
      
    
    
    
      
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Lato:300,400,400i,700%7CUbuntu+Mono&display=fallback">
        <style>:root{--md-text-font-family:"Lato";--md-code-font-family:"Ubuntu Mono"}</style>
      
    
    
      <link rel="manifest" href="../manifest.webmanifest" crossorigin="use-credentials">
    
    
      <link rel="stylesheet" href="../css/extra.css">
    
    
      
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="" data-md-color-primary="white" data-md-color-accent="white">
      
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#black-box-recursive-translations-for-molecular-optimization" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
      

<header class="md-header" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="https://www.stephenra.com" title="Stephen Ra" class="md-header__button md-logo" aria-label="Stephen Ra">
      
  <img src="../img/logo.png" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3V6m0 5h18v2H3v-2m0 5h18v2H3v-2z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            Stephen Ra
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Black box recursive translations for molecular optimization
            
          </span>
        </div>
      </div>
    </div>
    <div class="md-header__options">
      
    </div>
    
    
      <div class="md-header__source">
        
<a href="https://github.com/stephenra/stephenra.github.io/" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M439.55 236.05L244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    stephenra/stephenra.github.io
  </div>
</a>
      </div>
    
  </nav>
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    




<nav class="md-nav md-nav--primary" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="https://www.stephenra.com" title="Stephen Ra" class="md-nav__button md-logo" aria-label="Stephen Ra">
      
  <img src="../img/logo.png" alt="logo">

    </a>
    Stephen Ra
  </label>
  
    <div class="md-nav__source">
      
<a href="https://github.com/stephenra/stephenra.github.io/" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><path d="M439.55 236.05L244 40.45a28.87 28.87 0 0 0-40.81 0l-40.66 40.63 51.52 51.52c27.06-9.14 52.68 16.77 43.39 43.68l49.66 49.66c34.23-11.8 61.18 31 35.47 56.69-26.49 26.49-70.21-2.87-56-37.34L240.22 199v121.85c25.3 12.54 22.26 41.85 9.08 55a34.34 34.34 0 0 1-48.55 0c-17.57-17.6-11.07-46.91 11.25-56v-123c-20.8-8.51-24.6-30.74-18.64-45L142.57 101 8.45 235.14a28.86 28.86 0 0 0 0 40.81l195.61 195.6a28.86 28.86 0 0 0 40.8 0l194.69-194.69a28.86 28.86 0 0 0 0-40.81z"/></svg>
  </div>
  <div class="md-source__repository">
    stephenra/stephenra.github.io
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_1" type="checkbox" id="__nav_1" >
      
      <label class="md-nav__link" for="__nav_1">
        Latest
        <span class="md-nav__icon md-icon"></span>
      </label>
      <nav class="md-nav" aria-label="Latest" data-md-level="1">
        <label class="md-nav__title" for="__nav_1">
          <span class="md-nav__icon md-icon"></span>
          Latest
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
  
  
  
    <li class="md-nav__item">
      <a href=".." class="md-nav__link">
        Black box recursive translations for molecular optimization
      </a>
    </li>
  

          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2" type="checkbox" id="__nav_2" checked>
      
      <label class="md-nav__link" for="__nav_2">
        Blog
        <span class="md-nav__icon md-icon"></span>
      </label>
      <nav class="md-nav" aria-label="Blog" data-md-level="1">
        <label class="md-nav__title" for="__nav_2">
          <span class="md-nav__icon md-icon"></span>
          Blog
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
  
  
    
  
  
    
    <li class="md-nav__item md-nav__item--active md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2_1" type="checkbox" id="__nav_2_1" checked>
      
      <label class="md-nav__link" for="__nav_2_1">
        2020
        <span class="md-nav__icon md-icon"></span>
      </label>
      <nav class="md-nav" aria-label="2020" data-md-level="2">
        <label class="md-nav__title" for="__nav_2_1">
          <span class="md-nav__icon md-icon"></span>
          2020
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
  
  
    
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" data-md-toggle="toc" type="checkbox" id="__toc">
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          Black box recursive translations for molecular optimization
          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        Black box recursive translations for molecular optimization
      </a>
      
        
<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#introduction" class="md-nav__link">
    Introduction
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#related-work" class="md-nav__link">
    Related work
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#molecular-optimization-as-a-translation-problem" class="md-nav__link">
    Molecular optimization as a translation problem
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#black-box-recursive-translation-bbrt" class="md-nav__link">
    Black Box Recursive Translation (BBRT)
  </a>
  
    <nav class="md-nav" aria-label="Black Box Recursive Translation (BBRT)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#current-inference-methods" class="md-nav__link">
    Current inference methods
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#recursive-inference" class="md-nav__link">
    Recursive inference
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#experiments" class="md-nav__link">
    Experiments
  </a>
  
    <nav class="md-nav" aria-label="Experiments">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#molecule-generation-results" class="md-nav__link">
    Molecule generation results
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#empirical-properties-of-recursive-translation" class="md-nav__link">
    Empirical properties of recursive translation
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#interpretable-user-centric-optimization" class="md-nav__link">
    Interpretable, user-centric optimization
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#improving-secondary-properties-by-ranking" class="md-nav__link">
    Improving secondary properties by ranking
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#future-work" class="md-nav__link">
    Future work
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#appendix" class="md-nav__link">
    Appendix
  </a>
  
    <nav class="md-nav" aria-label="Appendix">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#a-recursive-penalized-logp-experiments" class="md-nav__link">
    A. Recursive penalized logP experiments
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#b-supplemental-experiments" class="md-nav__link">
    B. Supplemental Experiments
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

          
        </ul>
      </nav>
    </li>
  

          
            
  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_2_2" type="checkbox" id="__nav_2_2" >
      
      <label class="md-nav__link" for="__nav_2_2">
        2019
        <span class="md-nav__icon md-icon"></span>
      </label>
      <nav class="md-nav" aria-label="2019" data-md-level="2">
        <label class="md-nav__title" for="__nav_2_2">
          <span class="md-nav__icon md-icon"></span>
          2019
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
  
  
  
    <li class="md-nav__item">
      <a href="../05-07-19-adaptive-sampling/" class="md-nav__link">
        On conditioning by adaptive sampling
      </a>
    </li>
  

          
        </ul>
      </nav>
    </li>
  

          
        </ul>
      </nav>
    </li>
  

    
      
      
      

  
  
  
    
    <li class="md-nav__item md-nav__item--nested">
      
      
        <input class="md-nav__toggle md-toggle" data-md-toggle="__nav_3" type="checkbox" id="__nav_3" >
      
      <label class="md-nav__link" for="__nav_3">
        About
        <span class="md-nav__icon md-icon"></span>
      </label>
      <nav class="md-nav" aria-label="About" data-md-level="1">
        <label class="md-nav__title" for="__nav_3">
          <span class="md-nav__icon md-icon"></span>
          About
        </label>
        <ul class="md-nav__list" data-md-scrollfix>
          
            
  
  
  
    <li class="md-nav__item">
      <a href="../about/" class="md-nav__link">
        About Me
      </a>
    </li>
  

          
        </ul>
      </nav>
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    
<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#introduction" class="md-nav__link">
    Introduction
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#related-work" class="md-nav__link">
    Related work
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#molecular-optimization-as-a-translation-problem" class="md-nav__link">
    Molecular optimization as a translation problem
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#black-box-recursive-translation-bbrt" class="md-nav__link">
    Black Box Recursive Translation (BBRT)
  </a>
  
    <nav class="md-nav" aria-label="Black Box Recursive Translation (BBRT)">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#current-inference-methods" class="md-nav__link">
    Current inference methods
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#recursive-inference" class="md-nav__link">
    Recursive inference
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#experiments" class="md-nav__link">
    Experiments
  </a>
  
    <nav class="md-nav" aria-label="Experiments">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#molecule-generation-results" class="md-nav__link">
    Molecule generation results
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#empirical-properties-of-recursive-translation" class="md-nav__link">
    Empirical properties of recursive translation
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#interpretable-user-centric-optimization" class="md-nav__link">
    Interpretable, user-centric optimization
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#improving-secondary-properties-by-ranking" class="md-nav__link">
    Improving secondary properties by ranking
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#future-work" class="md-nav__link">
    Future work
  </a>
  
</li>
      
        <li class="md-nav__item">
  <a href="#appendix" class="md-nav__link">
    Appendix
  </a>
  
    <nav class="md-nav" aria-label="Appendix">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#a-recursive-penalized-logp-experiments" class="md-nav__link">
    A. Recursive penalized logP experiments
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#b-supplemental-experiments" class="md-nav__link">
    B. Supplemental Experiments
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          <div class="md-content" data-md-component="content">
            <article class="md-content__inner md-typeset">
              
                
                  <a href="https://github.com/stephenra/stephenra.github.io/edit/master/docs/01-06-20-bbrt.md" title="Edit this page" class="md-content__button md-icon">
                    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20.71 7.04c.39-.39.39-1.04 0-1.41l-2.34-2.34c-.37-.39-1.02-.39-1.41 0l-1.84 1.83 3.75 3.75M3 17.25V21h3.75L17.81 9.93l-3.75-3.75L3 17.25z"/></svg>
                  </a>
                
                
                <h1 id="black-box-recursive-translations-for-molecular-optimization">Black Box Recursive Translations for Molecular Optimization</h1>
<p>For the first post of 2020, I wanted to highlight our last contribution of 2019 -- work done by <a href="http://fdamani.com/">Farhan Damani</a>, a Ph.D. student with <a href="https://www.cs.princeton.edu/~rpa/">Ryan Adams</a> at Princeton University, my colleague, <a href="https://dbgroup.mit.edu/vishnu-sresht-0">Vishnu Sresht</a>, and myself -- where we cast molecular optimization as a translation problem, where the goal is to map an input compound to a target compound with improved biochemical properties. We observe that when generated molecules are iteratively fed back into the translator, molecular compound attributes improve with each step. We demonstrate that this finding is invariant to the choice of translation model, making this a "black box" algorithm. We call this method <strong>Black Box Recursive Translation (BBRT)</strong>, a new inference method for molecular property optimization. This simple, powerful technique operates strictly on the inputs and outputs of any translation model. We obtain new state-of-the-art results for molecular property optimization tasks using our simple drop-in replacement with well-known sequence and graph-based models. Our method provides a significant boost in performance relative to its non-recursive peers with just a simple <code>for</code> loop. Lastly, BBRT is highly interpretable, allowing users to map the evolution of newly discovered compounds from known starting points.</p>
<p>The preprint is now available on <a href="https://arxiv.org/pdf/1912.10156.pdf">arXiv</a>.</p>
<hr />
<h2 id="introduction">Introduction</h2>
<p>Automated molecular design using generative models offers the promise of rapidly discovering new compounds with desirable properties. Chemical space is large, discrete, and unstructured, which together, present important challenges to the success of any molecular optimization campaign. Approximately <span><span class="MathJax_Preview">10^8</span><script type="math/tex">10^8</script></span> compounds have been synthesized<sup id="fnref:1"><a class="footnote-ref" href="#fn:1">1</a></sup> while the range of potential drug-like candidates is estimated to between <span><span class="MathJax_Preview">10^{23}</span><script type="math/tex">10^{23}</script></span> and <span><span class="MathJax_Preview">10^{80}</span><script type="math/tex">10^{80}</script></span><sup id="fnref:2"><a class="footnote-ref" href="#fn:2">2</a></sup>. Consequently, new methods for intelligent search are paramount.</p>
<p>A recently introduced paradigm for compound generation treats molecular optimization as a translation task where the goal is to map an input compound to a target compound with favorable properties<sup id="fnref:3"><a class="footnote-ref" href="#fn:3">3</a></sup>. This framework has presented impressive results for constrained molecular property optimization where generated compounds are restricted
to be structurally similar to the source molecule.</p>
<p>We extend this framework to unconstrained molecular optimization by treating inference, vis-à-vis decoding strategies, as a first-class citizen. We observe that generated molecules can be repeatedly fed back into the model to generate even better compounds. This finding is
invariant to the choice of translation model, making this a “black box” algorithm. This invariance is particularly attractive considering the recent emphasis on new molecular representations<sup id="fnref:4"><a class="footnote-ref" href="#fn:4">4</a></sup> <sup id="fnref:5"><a class="footnote-ref" href="#fn:5">5</a></sup> <sup id="fnref:6"><a class="footnote-ref" href="#fn:6">6</a></sup> <sup id="fnref:7"><a class="footnote-ref" href="#fn:7">7</a></sup> <sup id="fnref:8"><a class="footnote-ref" href="#fn:8">8</a></sup> <sup id="fnref:9"><a class="footnote-ref" href="#fn:9">9</a></sup>. Using our simple drop-in replacement, our method can leverage these recently introduced molecular representations in a translation setting for better optimization.</p>
<p>We introduce <strong>Black Box Recursive Translation (BBRT)</strong>, a new inference method for molecular property optimization:</p>
<p><img src="../img/bbrt-diagram.png" alt="bbrt-diagram" width="700"/></p>
<p><center><small><b>Figure 1:</b> Black Box Recursive Translation (BBRT)</small></center></p>
<p><strong>Surprisingly, by applying BBRT to well-known sequence- and graph-based models in the literature, we can produce new state-of-the-art results on property optimization
benchmark tasks.</strong> Through an exhaustive exploration of various decoding strategies, we demonstrate the empirical benefits of using BBRT. We introduce simple ranking methods to decide which outputs are fed back into the model and find ranking to be an appealing approach to secondary property optimization. Finally, we demonstrate how BBRT is an
extensible tool for interpretable and user-centric molecular design applications.</p>
<h2 id="related-work">Related work</h2>
<p><strong><em>De novo</em> molecular design</strong>. Recent work has focused on learning new molecular representations including graphs<sup id="fnref:10"><a class="footnote-ref" href="#fn:10">10</a></sup> <sup id="fnref2:7"><a class="footnote-ref" href="#fn:7">7</a></sup>, grammars<sup id="fnref2:8"><a class="footnote-ref" href="#fn:8">8</a></sup> <sup id="fnref2:6"><a class="footnote-ref" href="#fn:6">6</a></sup>, trees<sup id="fnref2:5"><a class="footnote-ref" href="#fn:5">5</a></sup>, and sequences<sup id="fnref2:4"><a class="footnote-ref" href="#fn:4">4</a></sup> <sup id="fnref2:9"><a class="footnote-ref" href="#fn:9">9</a></sup>. Provided with a molecular representation, latent variable models<sup id="fnref3:8"><a class="footnote-ref" href="#fn:8">8</a></sup> <sup id="fnref3:6"><a class="footnote-ref" href="#fn:6">6</a></sup> <sup id="fnref3:5"><a class="footnote-ref" href="#fn:5">5</a></sup>, Markov chains <sup id="fnref:11"><a class="footnote-ref" href="#fn:11">11</a></sup>, or autoregressive models<sup id="fnref2:10"><a class="footnote-ref" href="#fn:10">10</a></sup> have been developed to learn distributions over molecular data. Molecular optimization has been approached with reinforcement learning<sup id="fnref:12"><a class="footnote-ref" href="#fn:12">12</a></sup> <sup id="fnref:13"><a class="footnote-ref" href="#fn:13">13</a></sup> and optimization in continuous, learned latent spaces<sup id="fnref3:4"><a class="footnote-ref" href="#fn:4">4</a></sup>. For sequences more generally, Mueller et al. (2017)<sup id="fnref:14"><a class="footnote-ref" href="#fn:14">14</a></sup> perform constrained optimization in a latent space to improve the sentiment of source sentences.</p>
<p>We build on recent work introducing a third paradigm for design, focusing on molecular optimization as a translation problem <sup id="fnref2:3"><a class="footnote-ref" href="#fn:3">3</a></sup>, where molecules are optimized by translating from a source graph to a target graph. While retaining high similarity, the target graph improves on the source graph’s biochemical properties. With many ways to modify a molecule, Jin et al. (2019)<sup id="fnref3:3"><a class="footnote-ref" href="#fn:3">3</a></sup> use stochastic hidden states coupled with a left-to-right greedy decoder to generate multi-modal outputs. We extend the translation framework from similarity-constrained molecular optimization to the unconstrained setting of finding the <em>best</em> scoring molecules according to a given biochemical property. We show that while their translation model is not fundamentally limited to constrained optimization, their inference method restricts the framework’s application to more general problems.</p>
<p><strong>Matched molecular pair (MMP) analysis</strong>. MMP analysis is a popular cheminformatics framework for analyzing structure-property relationships<sup id="fnref:15"><a class="footnote-ref" href="#fn:15">15</a></sup> <sup id="fnref:16"><a class="footnote-ref" href="#fn:16">16</a></sup>. MMPs are extracted by mining databases of
measured chemical properties and identifying pairs of chemical structures that share the same core and differ only by a small, well-defined structural difference; for example, where a methyl group is
replaced by an isopropyl group<sup id="fnref:17"><a class="footnote-ref" href="#fn:17">17</a></sup>. The central hypothesis underlying MMPs is that the chemical properties of a series of closely related structures can be described by piecewise-independent
contributions that various structural adducts make to the properties of the core.</p>
<p>MMP analysis has become a mainstream tool for interpreting the complex landscape of structure-activity relationships via simple, local perturbations that can be learnt and potentially transferred across drug
design projects<sup id="fnref:18"><a class="footnote-ref" href="#fn:18">18</a></sup>. This framework serves as the chemistry analogue to an interpretability technique in machine learning called local interpretable model-agnostic explanations (LIME)<sup id="fnref:19"><a class="footnote-ref" href="#fn:19">19</a></sup>.
Both MMP and LIME learn local surrogate models to explain individual predictions.</p>
<p>We view molecular translation as a learned MMP analysis. While Jin et al. (2019)<sup id="fnref4:3"><a class="footnote-ref" href="#fn:3">3</a></sup> use neural networks to learn a single high-dimensional MMP step, we extend this framework to infer a sequence of MMPs, extending the reach of this framework to problems beyond constrained optimization.</p>
<p><strong>Translation models.</strong> Machine translation models composed of end-to-end neural networks<sup id="fnref:20"><a class="footnote-ref" href="#fn:20">20</a></sup> have enjoyed significant success as a general-purpose modeling tool for many applications including dialogue generation<sup id="fnref:21"><a class="footnote-ref" href="#fn:21">21</a></sup> and image captioning<sup id="fnref:22"><a class="footnote-ref" href="#fn:22">22</a></sup>. We focus on a recently introduced
application of translation modeling, one of molecular optimization<sup id="fnref5:3"><a class="footnote-ref" href="#fn:3">3</a></sup>.</p>
<p>The standard approach to inference – approximately decoding the most likely sequence under the model – involves a left-to-right greedy search, which is known to be highly suboptimal, producing generic
sequences exhibiting low diversity<sup id="fnref:23"><a class="footnote-ref" href="#fn:23">23</a></sup>. Recent work propose diverse beam search<sup id="fnref:24"><a class="footnote-ref" href="#fn:24">24</a></sup> <sup id="fnref:25"><a class="footnote-ref" href="#fn:25">25</a></sup> <sup id="fnref:26"><a class="footnote-ref" href="#fn:26">26</a></sup>, sampling methods geared towards open-ended tasks<sup id="fnref:27"><a class="footnote-ref" href="#fn:27">27</a></sup> <sup id="fnref2:23"><a class="footnote-ref" href="#fn:23">23</a></sup>, and reinforcement learning for post-hoc secondary objective optimization<sup id="fnref:28"><a class="footnote-ref" href="#fn:28">28</a></sup> <sup id="fnref:29"><a class="footnote-ref" href="#fn:29">29</a></sup> <sup id="fnref:30"><a class="footnote-ref" href="#fn:30">30</a></sup>. Motivated by molecular optimization as a translation task, we develop Black Box Recursive Translation (BBRT). We show BBRT generates samples with better molecular properties than its non-recursive peers for both deterministic and stochastic decoding strategies.</p>
<h2 id="molecular-optimization-as-a-translation-problem">Molecular optimization as a translation problem</h2>
<p>For illustrative purposes, we describe the translation framework and the corresponding inference method for sequences. We emphasize that our inference method is a black box, which means it is invariant to specific architectural and representational choices.</p>
<p><strong>Background.</strong> Our goal is to optimize the chemical properties of molecules using a sequence-based molecular representation. We are given <span><span class="MathJax_Preview">(x,y) \in (\mathcal{X}, \mathcal{Y})</span><script type="math/tex">(x,y) \in (\mathcal{X}, \mathcal{Y})</script></span> as a sequence pair, where
<span><span class="MathJax_Preview">x = (x_1, x_2, \dots, x_m)</span><script type="math/tex">x = (x_1, x_2, \dots, x_m)</script></span> is the source sequence with <span><span class="MathJax_Preview">m</span><script type="math/tex">m</script></span> tokens and <span><span class="MathJax_Preview">y = (y_1, y_2, \dots, y_n)</span><script type="math/tex">y = (y_1, y_2, \dots, y_n)</script></span> is the target sequence with <span><span class="MathJax_Preview">n</span><script type="math/tex">n</script></span> tokens, and <span><span class="MathJax_Preview">\mathcal{X}</span><script type="math/tex">\mathcal{X}</script></span> and <span><span class="MathJax_Preview">\mathcal{Y}</span><script type="math/tex">\mathcal{Y}</script></span> are the source and target domains
respectively. We focus on problems where the source and target domains are identical. By construction, our training pairs <span><span class="MathJax_Preview">(x,y)</span><script type="math/tex">(x,y)</script></span> have high chemical similarity, which helps the model learn local edits to <span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span>.
Additionally, <span><span class="MathJax_Preview">y</span><script type="math/tex">y</script></span> always scores higher than <span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> on the property to be optimized. These properties are specified beforehand when constructing training pairs. A single task will typically optimize a single property such as potency, toxicity, or lipophilic efficiency.</p>
<p>A sequence to sequence (Seq2Seq) model <sup id="fnref2:20"><a class="footnote-ref" href="#fn:20">20</a></sup> learns parameters <span><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span> that estimate a conditional probability model <span><span class="MathJax_Preview">P(y|x; \theta)</span><script type="math/tex">P(y|x; \theta)</script></span>, where <span><span class="MathJax_Preview">\theta</span><script type="math/tex">\theta</script></span> is estimated by maximizing the log-likelihood:</p>
<script type="math/tex; mode=display">
L(\theta) = \sum_{(x,y) \in (\mathcal{X}, \mathcal{Y})} \log P(y|x,\theta)
</script>

<p>The conditional probability is typically factorized according to the Chain Rule:</p>
<script type="math/tex; mode=display">
P(y|x; \theta) = \prod_{t=1}^n P(y_t | y_{<t}, x, \theta)
</script>

<p>These models use an encoder-decoder architecture where the input and output are both parameterized by recurrent neural networks (RNNs). The encoder reads the source sequence <span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span> and generates a sequence of hidden representations. The decoder estimates the conditional probability of each target token given the source representations and its preceding tokens. The attention mechanism<sup id="fnref:31"><a class="footnote-ref" href="#fn:31">31</a></sup> helps with token
generation by focusing on token-specific source representations.</p>
<h2 id="black-box-recursive-translation-bbrt">Black Box Recursive Translation (BBRT)</h2>
<h3 id="current-inference-methods">Current inference methods</h3>
<p>For translation models, the inference task is to compute <span><span class="MathJax_Preview">y^* = {\operatorname{argmax}} p(y|x,\theta)</span><script type="math/tex">y^* = {\operatorname{argmax}} p(y|x,\theta)</script></span>. Because the search space of potential outputs is large, in practice we can only explore a limited number of
sequences. Given a fixed computational budget, likely sequences are typically generated with heuristics. Decoding methods can be classified as deterministic or stochastic. We now describe both classes of decoding
strategies in detail. For this section, we follow the notation described in Welleck et al. (2019)<sup id="fnref:32"><a class="footnote-ref" href="#fn:32">32</a></sup>.</p>
<p><strong>Deterministic decoding.</strong> Two popular deterministic methods include
greedy search and beam search<sup id="fnref:33"><a class="footnote-ref" href="#fn:33">33</a></sup> <sup id="fnref:34"><a class="footnote-ref" href="#fn:34">34</a></sup>. Greedy search performs a
single left-to-right pass, selecting the most likely token at each time step: <span><span class="MathJax_Preview">y_t = {\operatorname{argmax}} p(y_t|y_{&lt;t}, x,\theta)</span><script type="math/tex">y_t = {\operatorname{argmax}} p(y_t|y_{<t}, x,\theta)</script></span>. While this method is efficient, it leads to suboptimal generation as it does not take into account the future when selecting tokens.</p>
<p>Beam search is a generalization of greedy search and maintains a set of <span><span class="MathJax_Preview">k</span><script type="math/tex">k</script></span> hypotheses at each time-step where each hypothesis is a partially decoded sequence. While in machine translation beam search variants are
the preferred method, for more open-ended tasks, beam search fails to generate diverse candidates. Recent work has explored diverse beam search<sup id="fnref2:24"><a class="footnote-ref" href="#fn:24">24</a></sup> <sup id="fnref2:25"><a class="footnote-ref" href="#fn:25">25</a></sup> <sup id="fnref2:26"><a class="footnote-ref" href="#fn:26">26</a></sup>. These methods
address the reduction of number of duplicate sequences to varying extents, thereby increasing the entropy of generated sequences.</p>
<p><strong>Stochastic decoding.</strong> A separate class of decoding methods sample from the model at generation time,
<span><span class="MathJax_Preview">y_t \sim q(y_t | y_{&lt;t}, x, p_{\theta})</span><script type="math/tex">y_t \sim q(y_t | y_{<t}, x, p_{\theta})</script></span>. This method has shown to be effective at generating diverse samples and can better explore target design spaces. We consider a top-<span><span class="MathJax_Preview">k</span><script type="math/tex">k</script></span> sampler<sup id="fnref2:27"><a class="footnote-ref" href="#fn:27">27</a></sup>, which
restricts sampling to the <span><span class="MathJax_Preview">k</span><script type="math/tex">k</script></span> most-probable tokens at time-step <span><span class="MathJax_Preview">t</span><script type="math/tex">t</script></span>. This corresponds to restricting sampling to a subset of the vocabulary <span><span class="MathJax_Preview">U \subset V</span><script type="math/tex">U \subset V</script></span>. <span><span class="MathJax_Preview">U</span><script type="math/tex">U</script></span> is the subset of <span><span class="MathJax_Preview">V</span><script type="math/tex">V</script></span> that maximizes <span><span class="MathJax_Preview">\sum_{y \in U} p_{\theta}(y_t | y_{y&lt;t}, x)</span><script type="math/tex">\sum_{y \in U} p_{\theta}(y_t | y_{y<t}, x)</script></span>:</p>
<p><script type="math/tex; mode=display">
q(y_t | y_{&lt;t}, x, p_{\theta}) =  <script type="math/tex; mode=display">\begin{cases} 
      \frac{p_{\theta}(y_t | y_{<t}, x)}{Z} & y_t \in U \\
      0 & \text{otherwise}
   \end{cases}</script>
</script></p>
<h3 id="recursive-inference">Recursive inference</h3>
<p>We are given <span><span class="MathJax_Preview">(x,y) \in (X,Y)</span><script type="math/tex">(x,y) \in (X,Y)</script></span> as a sequence pair where by construction
<span><span class="MathJax_Preview">(x,y)</span><script type="math/tex">(x,y)</script></span> has high chemical similarity and <span><span class="MathJax_Preview">y</span><script type="math/tex">y</script></span> scores higher on a
prespecified property compared to <span><span class="MathJax_Preview">x</span><script type="math/tex">x</script></span>. At test time, we are interested
in recursively inferring new sequences. Let <span><span class="MathJax_Preview">y_i</span><script type="math/tex">y_i</script></span> denote a random
sequence for recursive iteration <span><span class="MathJax_Preview">i</span><script type="math/tex">i</script></span>. Let <span><span class="MathJax_Preview">\{y_i^{(k)}\}_{k=1}^K</span><script type="math/tex">\{y_i^{(k)}\}_{k=1}^K</script></span> be a
set of <span><span class="MathJax_Preview">K</span><script type="math/tex">K</script></span> outputs generated from <span><span class="MathJax_Preview">p_{\theta}(y_i|x)</span><script type="math/tex">p_{\theta}(y_i|x)</script></span> when <span><span class="MathJax_Preview">i=0</span><script type="math/tex">i=0</script></span>. We use
a scoring function <span><span class="MathJax_Preview">S</span><script type="math/tex">S</script></span> to compute the best of <span><span class="MathJax_Preview">K</span><script type="math/tex">K</script></span> outputs denoted as
<span><span class="MathJax_Preview">\hat{y}_i</span><script type="math/tex">\hat{y}_i</script></span>. For <span><span class="MathJax_Preview">i&gt;0</span><script type="math/tex">i>0</script></span>, we infer <span><span class="MathJax_Preview">K</span><script type="math/tex">K</script></span> outputs from
<span><span class="MathJax_Preview">p_{\theta}(y_i|\hat{y}_{i-1})</span><script type="math/tex">p_{\theta}(y_i|\hat{y}_{i-1})</script></span>. This process is repeated for <span><span class="MathJax_Preview">n</span><script type="math/tex">n</script></span>
iterations.</p>
<p><strong>Scoring functions.</strong> In principle, all <span><span class="MathJax_Preview">K</span><script type="math/tex">K</script></span> outputs at iteration <span><span class="MathJax_Preview">i</span><script type="math/tex">i</script></span>
can be independently conditioned on to generate new candidates for
iteration <span><span class="MathJax_Preview">i+1</span><script type="math/tex">i+1</script></span>. This procedure scales exponentially with respect to
space and time as a function of <span><span class="MathJax_Preview">n</span><script type="math/tex">n</script></span> iterations. Therefore, we introduce
a suite of simple ranking strategies to score <span><span class="MathJax_Preview">K</span><script type="math/tex">K</script></span> output sequences to
decide which output becomes the next iteration’s source sequence. We
consider a likelihood based ranking as well as several
chemistry-specific metrics further described in the experiments.
Designing well-informed scoring functions can help calibrate the
distributional statistics of generated sequences, aid in multi-property
optimization, and provide interpretable sequences of iteratively optimal
translations.</p>
<p><strong>Ensemble outputs.</strong> After <span><span class="MathJax_Preview">n</span><script type="math/tex">n</script></span> recursive iterations, we ensemble the
generated outputs <span><span class="MathJax_Preview">\{y_0, y_1, \dots, y_n\}_{k=1}^K</span><script type="math/tex">\{y_0, y_1, \dots, y_n\}_{k=1}^K</script></span> and score the
sequences on a desired objective. For property optimization, we return
the <span><span class="MathJax_Preview">\operatorname{argmax}</span><script type="math/tex">\operatorname{argmax}</script></span>. In principle, BBRT is not limited to ensembling recursive
outputs from a <em>single</em> model. Different modeling choices and molecular
representations have different inductive biases, which influence the
diversity of generated compounds. BBRT can capitalize on these
differences by providing a coherent method to aggregate results.</p>
<h2 id="experiments">Experiments</h2>
<p>We apply BBRT to solve unconstrained and multi-property optimization
problems. To highlight the generality of our method, we apply recursive
translation to both sequence and graph-based translation models. <strong>We
show that BBRT generates state-of-the-art results on molecular property
optimization using already published modeling approaches</strong>. Next we
describe how recursive inference lends itself to interpretability
through the generation of molecular traces, allowing practitioners to
map the evolution of discovered compounds from known starting points
through a sequence of local structural changes. At any point in a
molecular trace, users may introduce a "break point" to consider
alternative translations thereby personally evaluating the trade-offs
between conflicting design objectives. Finally, we apply BBRT to the
problem of secondary property optimization by ranking.</p>
<p><strong>Models.</strong> We apply our inference method to sequence and graph-based
molecular representations. For sequences, we use the recently introduced
SELFIES molecular representation<sup id="fnref3:9"><a class="footnote-ref" href="#fn:9">9</a></sup>, a sequence-based
representation for semantically constrained graphs. Empirically, this
method generates a high percentage of valid compounds<sup id="fnref4:9"><a class="footnote-ref" href="#fn:9">9</a></sup>.
Using SELFIES, we develop a Seq2Seq model with an encoder-decoder
framework. The encoder and decoder are both parameterized by RNNs with
Long Short-Term Memory (LSTM) cells<sup id="fnref:35"><a class="footnote-ref" href="#fn:35">35</a></sup>. The encoder is a 2-layer
bidirectional RNN and the decoder is a 1-layer unidirectional forward
RNN. We also use attention<sup id="fnref2:31"><a class="footnote-ref" href="#fn:31">31</a></sup> for decoding. The hidden
representations are non-probabilistic and are optimized to minimize a
standard cross-entropy loss with teacher forcing. Decoding is performed
using deterministic and stochastic decoding strategies described in .
For graphs, we use a tree-based molecular representation<sup id="fnref4:5"><a class="footnote-ref" href="#fn:5">5</a></sup>
with the exact architecture described in Jin et al. 2019<sup id="fnref6:3"><a class="footnote-ref" href="#fn:3">3</a></sup>. Decoding is
performed using a probabilistic extension with latent variables
described in Jin et al. 2019<sup id="fnref7:3"><a class="footnote-ref" href="#fn:3">3</a></sup> -- we sample from the prior <span><span class="MathJax_Preview">k</span><script type="math/tex">k</script></span> times followed by left-to-right greedy decoding.</p>
<p><strong>Data</strong>. We construct training data by sampling molecular pairs <span><span class="MathJax_Preview">(X,Y)</span><script type="math/tex">(X,Y)</script></span>
with molecular similarity <span><span class="MathJax_Preview">sim(X,Y) &gt; \tau</span><script type="math/tex">sim(X,Y) > \tau</script></span> and property improvement
<span><span class="MathJax_Preview">\delta(Y) &gt; \delta(X)</span><script type="math/tex">\delta(Y) > \delta(X)</script></span> for a given property <span><span class="MathJax_Preview">\delta</span><script type="math/tex">\delta</script></span>. Constructing
training pairs with a similarity constraint can help avoid degenerate
mappings. In contrast to Jin et al. (2019)<sup id="fnref8:3"><a class="footnote-ref" href="#fn:3">3</a></sup>, we only enforce the similarity
constraint for the construction of training pairs. Similarity
constraints are not enforced at test time. Molecular similarity is
measured by computing Tanimoto similarity with Morgan fingerprints
<sup id="fnref:36"><a class="footnote-ref" href="#fn:36">36</a></sup>. All models were trained on the open-source ZINC
dataset<sup id="fnref:37"><a class="footnote-ref" href="#fn:37">37</a></sup>. We use the ZINC-250K subset, as described in
Gómez-Bombarelli et al. (2018)<sup id="fnref4:4"><a class="footnote-ref" href="#fn:4">4</a></sup>.</p>
<p><strong>Properties.</strong> For all experiments, we focus on optimizing two
well-known drug properties of molecules. First, we optimize the
water-octanol partition coefficient (logP). Similar to Jin et al. (2018)<sup id="fnref5:5"><a class="footnote-ref" href="#fn:5">5</a></sup>, Kusner et al. (2017)<sup id="fnref4:8"><a class="footnote-ref" href="#fn:8">8</a></sup>, and Gómez-Bombarelli et al. (2018)<sup id="fnref5:4"><a class="footnote-ref" href="#fn:4">4</a></sup>, we consider a penalized logP
score that incorporates ring size and synthetic accessibility. The
penalized logP score uses a dataset normalization score described in
You et al. (2018)<sup id="fnref:38"><a class="footnote-ref" href="#fn:38">38</a></sup>. Following Jin et al. (2019)<sup id="fnref9:3"><a class="footnote-ref" href="#fn:3">3</a></sup> we extracted 99K translation pairs
for training using a similarity constraint of 0.4. Second, we optimize
quantitative estimate of drug likeness (QED)<sup id="fnref:39"><a class="footnote-ref" href="#fn:39">39</a></sup>. Following Jin et al. (2019)<sup id="fnref10:3"><a class="footnote-ref" href="#fn:3">3</a></sup>, we construct
training pairs where the source compound has a QED score within the
range [0.7 0.8] and the target compound has a QED score within the range
[0.9 1.0]. While Jin et al. (2019)<sup id="fnref11:3"><a class="footnote-ref" href="#fn:3">3</a></sup> evaluates QED performance based on a closed
set translation task, we evaluate this model for unconstrained
optimization. We extracted a training set of 88K molecule pairs. We
report the details on how these properties were computed in <a href="#a-recursive-penalized-logp-experiments">Appendix A</a>.</p>
<p><strong>Scoring functions</strong>. Here we describe scoring functions that are used
to rank <span><span class="MathJax_Preview">K</span><script type="math/tex">K</script></span> outputs for recursive iteration <span><span class="MathJax_Preview">i</span><script type="math/tex">i</script></span>. The highest scoring
sequence is used as the next iteration’s source compound.</p>
<ul>
<li>
<p><code>Penalized logP</code>: Choose the compound with the maximum logP value.
    This is useful when optimizing logP as a primary or secondary
    property.</p>
</li>
<li>
<p><code>QED</code>: Choose the compound with the maximum QED value. This is useful
    when optimizing QED as a primary or secondary property.</p>
</li>
<li>
<p><code>Max Delta Sim</code>: Choose the compound with the highest chemical
    similarity to the previous iteration’s source compound. This is
    useful for interpretable, molecular traces by creating a sequence of
    translations with local edits.</p>
</li>
<li>
<p><code>Max Init Sim</code>: Choose the compound with the highest similarity to the
    initial seed compound. This is useful for input-constrained
    optimization.</p>
</li>
<li>
<p><code>Min Mol Wt</code>: Choose the compound with the minimum molecular weight.
    This is useful for rectifying a molecular generation artifact where
    models optimize logP by simply adding functional groups, thus
    increasing the molecular weight (Figure [fig:rank<sub>g</sub>2g]).</p>
</li>
</ul>
<p>Diversity is computed as follows. Let <span><span class="MathJax_Preview">Y</span><script type="math/tex">Y</script></span> be a set of translated compounds. Consistent with the literature<sup id="fnref:40"><a class="footnote-ref" href="#fn:40">40</a></sup> we report diversity as:</p>
<div>
<div class="MathJax_Preview">\text{DIV}(Y) = \frac{1}{|Y|(|Y|-1)} \sum_{y \in Y} \sum_{y' \in Y, y'} 1 - \delta(y,y')</div>
<script type="math/tex; mode=display">\text{DIV}(Y) = \frac{1}{|Y|(|Y|-1)} \sum_{y \in Y} \sum_{y' \in Y, y'} 1 - \delta(y,y')</script>
</div>
<p>where <span><span class="MathJax_Preview">\delta</span><script type="math/tex">\delta</script></span> is the Tanimoto similarity computed on the Morgan fingerprints of <span><span class="MathJax_Preview">y</span><script type="math/tex">y</script></span> and <span><span class="MathJax_Preview">y'</span><script type="math/tex">y'</script></span>.</p>
<h3 id="molecule-generation-results">Molecule generation results</h3>
<p><strong>Setup.</strong> For property optimization, the goal is to generate compounds
with the highest possible penalized logP and QED values. For notation,
we denote BBRT applied to model X as "BBRT-X". We consider BBRT-JTNN
and BBRT-Seq2Seq under 3 decoding strategies and 4 scoring functions.
For the logP task, we seed our translations with 900 maximally diverse
compounds with an average pairwise diversity of <span><span class="MathJax_Preview">0.94 \pm{0.04}</span><script type="math/tex">0.94 \pm{0.04}</script></span>
relative to <span><span class="MathJax_Preview">0.86 \pm{0.04}</span><script type="math/tex">0.86 \pm{0.04}</script></span>, which is the average pairwise diversity of
the training data. Maximally diverse compounds were computed using the
MaxMin algorithm <sup id="fnref:41"><a class="footnote-ref" href="#fn:41">41</a></sup> on Morgan fingerprints. We found seeding
our translations with diverse compounds helped BBRT generate higher
scoring compounds:</p>
<p><img src="../img/seed_choice.png" alt="bbrt-seed-choice" width="800"/></p>
<p><center><small><b>Figure 2:</b> <strong>(A)</strong>: Applying BBRT-Seq2Seq to three seed sequence sets with 100 samples each. The MaxMin algorithm was used to select samples with varying levels of diversity (max: 0.94, avg: 0.96, and min: 0.78). For each input sequence, we sampled 100 times from a top-5 sampler and ranked samples using logP. Standard error is reported by averaging over 10 BBRT-Seq2Seq runs with different seed sets. <strong>(B)</strong>: Applying BBRT-Seq2Seq to three seed sequence sets with 100 samples each and varying logP values (low: logP values &lt; 1, medium: [-1 1], high: values &gt; 1). Standard error is reported by averaging over 10 runs each with a randomly chosen set of seed sequences.</small></center></p>
<p>For both BBRT applications, we sampled 100 complete sequences from a top-<span><span class="MathJax_Preview">2</span><script type="math/tex">2</script></span> and from a top-<span><span class="MathJax_Preview">5</span><script type="math/tex">5</script></span> sampler and then aggregated these outputs with a beam search using 20 beams outputting 20 sequences.</p>
<p><strong>Baselines.</strong> We compare our method with the following state-of-the-art
baselines. Junction Tree VAE (JT-VAE)<sup id="fnref6:5"><a class="footnote-ref" href="#fn:5">5</a></sup> combines a
graph-based representation with latent variables for molecular graph
generation. Molecular optimization is performed using Bayesian
Optimization on the learned latent space to search for compounds with
optimized property scores. JT-VAE has been shown to outperform other
methods including CharacterVAE<sup id="fnref6:4"><a class="footnote-ref" href="#fn:4">4</a></sup>, GrammarVAE<sup id="fnref5:8"><a class="footnote-ref" href="#fn:8">8</a></sup>, and Syntax-Directed-VAE<sup id="fnref4:6"><a class="footnote-ref" href="#fn:6">6</a></sup>. We also compare against two reinforcement learning molecular generation
methods: Objective-Reinforced Generative Adversarial Networks (ORGAN)<sup id="fnref:42"><a class="footnote-ref" href="#fn:42">42</a></sup> uses SMILES strings <sup id="fnref:43"><a class="footnote-ref" href="#fn:43">43</a></sup>, a text-based molecular representation, and the Graph Convolutional Policy
Network (GCPN)<sup id="fnref2:38"><a class="footnote-ref" href="#fn:38">38</a></sup>, uses graphs.</p>
<p>We also compare against non-recursive variants of the Seq2Seq and
Variational Junction-Tree Encoder-Decoder (JTNN)<sup id="fnref12:3"><a class="footnote-ref" href="#fn:3">3</a></sup> models.
Seq2Seq is trained on the SELFIES representation<sup id="fnref5:9"><a class="footnote-ref" href="#fn:9">9</a></sup>. For a
fair comparison, we admit similar computational budgets to these
baselines. For Seq2Seq we include a deterministic and stochastic
decoding baseline. For the deterministic baseline, we use beam search
with 20 beams and compute the 20 most probable sequences under the
model. For the stochastic baseline, we sample 6600 times from a top-5
sampler. For details on the sampler, we refer readers to the <a href="#current-inference-methods">'Current inference methods'</a> section. For JTNN, we
use the reported <a href="https://github.com/wengong-jin/iclr19-graph2graph">GitHub implementation</a> from Jin et al. (2019)<sup id="fnref13:3"><a class="footnote-ref" href="#fn:3">3</a></sup>. There is not an
obvious stochastic and deterministic parallel considering their method
is probabilistic. Therefore, we focus on comparing to a fair
computational budget by sampling 6600 times from the prior distribution
followed by greedy decoding. For fair comparisons to the recursive
application, the same corresponding sampling strategies are used, with
100 samples per iteration.</p>
<p><strong>Results.</strong> Following reporting practices in the literature, we report
the top 3 property scores found by each model. <strong>Table 1</strong> summarizes these
results:</p>
<table>
<thead>
<tr>
<th align="left"></th>
<th align="center"></th>
<th align="center"><span style="color:white"><strong>Penalized logP</strong></span></th>
<th align="center"></th>
<th align="center"></th>
<th align="center"><span style="color:white"><strong>QED</strong></span></th>
<th align="center"></th>
</tr>
</thead>
<tbody>
<tr>
<td align="left"><strong>Method</strong></td>
<td align="center"><strong>1<sup>st</sup></strong></td>
<td align="center"><strong>2<sup>nd</sup></strong></td>
<td align="center"><strong>3<sup>rd</sup></strong></td>
<td align="center"><strong>1<sup>st</sup></strong></td>
<td align="center"><strong>2<sup>nd</sup></strong></td>
<td align="center"><strong>3<sup>rd</sup></strong></td>
</tr>
<tr>
<td align="left"><mark>ZINC-250K</mark></td>
<td align="center">4.52</td>
<td align="center">4.30</td>
<td align="center">4.23</td>
<td align="center">0.948</td>
<td align="center">0.948</td>
<td align="center">0.948</td>
</tr>
<tr>
<td align="left">ORGAN</td>
<td align="center">3.63</td>
<td align="center">3.49</td>
<td align="center">3.44</td>
<td align="center">0.896</td>
<td align="center">0.824</td>
<td align="center">0.820</td>
</tr>
<tr>
<td align="left">JT-VAE</td>
<td align="center">5.30</td>
<td align="center">4.93</td>
<td align="center">4.49</td>
<td align="center">0.925</td>
<td align="center">0.911</td>
<td align="center">0.910</td>
</tr>
<tr>
<td align="left">GCPN</td>
<td align="center">7.98</td>
<td align="center">7.85</td>
<td align="center">7.80</td>
<td align="center">0.948</td>
<td align="center">0.947</td>
<td align="center">0.946</td>
</tr>
<tr>
<td align="left">JTNN</td>
<td align="center">5.97</td>
<td align="center">4.96</td>
<td align="center">4.71</td>
<td align="center">0.948</td>
<td align="center">0.948</td>
<td align="center">0.948</td>
</tr>
<tr>
<td align="left">Seq2Seq</td>
<td align="center">4.65</td>
<td align="center">4.53</td>
<td align="center">4.49</td>
<td align="center">0.948</td>
<td align="center">0.948</td>
<td align="center">0.948</td>
</tr>
<tr>
<td align="left">BBRT-JTNN</td>
<td align="center"><mark><strong>10.13</strong></td>
<td align="center"><mark><strong>10.10</strong></td>
<td align="center"><mark><strong>9.91</strong></td>
<td align="center">0.948</td>
<td align="center">0.948</td>
<td align="center">0.948</td>
</tr>
<tr>
<td align="left">BBRT-Seq2Seq</td>
<td align="center">6.74</td>
<td align="center">6.47</td>
<td align="center">6.42</td>
<td align="center">0.948</td>
<td align="center">0.948</td>
<td align="center">0.948</td>
</tr>
</tbody>
</table>
<p><center><small><b>Table 1:</b> Top 3 property scores on penalized logP and QED tasks</small></center></p>
<p>The top 3 property scores found in ZINC-250k are included for
comparison. For logP optimization, BBRT-JTNN significantly outperforms
all baseline models including JTNN, Seq2Seq, and BBRT-Seq2Seq.
BBRT-Seq2Seq outperforms Seq2Seq, highlighting the benefits of recursive
inference for both molecular representations. For QED property
optimization, the two translation models and the BBRT variants all find
the same top 3 property scores, which is a new state-of-the-art result
for QED optimization.</p>
<p>In <strong>Figure 3</strong> (below), we report the top 100 logP compounds
generated by both BBRT applications relative to its non-recursive
counterparts and observe significant improvements in logP from using
BBRT. We also report a diversity measure of the generated candidates for
both BBRT models and the top 100 logP compounds in the training data.
The JTNN variant produces logP compounds that are more diverse than the
compounds in the training data, while the compounds generated by Seq2Seq
are less diverse:</p>
<p><img src="../img/top100logp.png" alt="bbrt-top100logp" width="700"/></p>
<p><center><small><b>Figure 3:</b> <strong>(Left and Center)</strong>: Top 100 logP generated compounds under BBRT-Seq2Seq, BBRT-JTNN, and their non-recursive counterparts. <strong>(Right)</strong>: Diversity of top 100 generated compounds under both BBRT models and the top 100 compounds from the training data.</small></center></p>
<p><strong>Figure 4</strong> visualizes the top 2 discovered compounds by
BBRT-JTNN and BBRT-Seq2Seq under both properties:</p>
<p><img src="../img/recursive_top2_comparison.png" alt="bbrt-top2-comparison" width="700"/></p>
<p><center><small><b>Figure 4:</b> Top scoring compounds for properties logP and QED under BBRT-Seq2Seq
and BBRT-JTNN</small></center></p>
<p>For logP, while BBRT-JTNN produces compounds with higher property values, BBRT-Seq2Seq’s
top 2 generated compounds have a richer molecular vocabulary.
BBRT-Seq2Seq generates compounds with heterocycles and linkages while
BBRT-JTNN generates a chain of linear hydrocarbons, which resemble the
top reported compounds in GCPN<sup id="fnref3:10"><a class="footnote-ref" href="#fn:10">10</a></sup>, an alternative
graph-based representation. The stark differences in the vocabulary from
the top scoring compounds generated by the sequence- and graph-based
representations highlight the importance of using flexible frameworks
that can ensemble results across molecular representations.</p>
<p><strong>Differences between logP and QED.</strong> For logP, BBRT provides a 27%
improvement over state-of-the-art for property optimization, while for
QED, despite recursive methods outperforming ORGAN, JT-VAE and GCPN, the
corresponding non-recursive techniques -- Seq2Seq and JTNN baselines --
perform just as well as with and without BBRT. We argue this might be a
result of these models reaching an upper bound for QED values,
motivating the importance of the community moving to new metrics in the
future<sup id="fnref:44"><a class="footnote-ref" href="#fn:44">44</a></sup>.</p>
<h3 id="empirical-properties-of-recursive-translation">Empirical properties of recursive translation</h3>
<p>We perform a sequence of ablation experiments to better understand the
effect of various BBRT design choices on performance. We highlight the
variability in average logP from translated outputs at each iteration
with different decoding strategies (<strong>Figure 5A. Left.</strong>)
and scoring functions (<strong>Figure 5A. Right.</strong>):</p>
<p><img src="../img/rec_inf_limit.png" alt="bbrt-rec_inf_limit" width="700"/>
<center><small><b>Figure 5:</b> Ablation experiments using BBRT-Seq2Seq. <strong>(A. Left):</strong> Mean LogP from 900 translations as a function of recursive iteration for three decoding strategies. Dotted lines denote non-recursive counterparts. <strong>(A. Right):</strong> Mean logP as a function of recursive iteration for 4 scoring functions. <strong>(B. Left):</strong> Diversity of generated outputs across recursive iterations for logP translation. <strong>(B. Right):</strong> Diversity of generated outputs across recursive iterations for QED translation. </small></center></p>
<p><strong>On the importance of stochastic decoding.</strong> For non-recursive and
recursive translation models, <strong>stochastic decoding methods outperformed
deterministic methods</strong> on average logP scores (<strong>Figure 5A. Left</strong>) average pairwise diversity (<strong>Figure 5B.</strong>) for generated compounds as a function of
recursive iteration. Non-greedy search strategies are not common
practice in <em>de novo</em> molecular design<sup id="fnref7:4"><a class="footnote-ref" href="#fn:4">4</a></sup> <sup id="fnref6:8"><a class="footnote-ref" href="#fn:8">8</a></sup> <sup id="fnref14:3"><a class="footnote-ref" href="#fn:3">3</a></sup>. While recent
work emphasizes novel network architectures and generating diverse
compounds using latent variables<sup id="fnref8:4"><a class="footnote-ref" href="#fn:4">4</a></sup> <sup id="fnref7:8"><a class="footnote-ref" href="#fn:8">8</a></sup> <sup id="fnref7:5"><a class="footnote-ref" href="#fn:5">5</a></sup>, we identify an
important design choice that typically has been underemphasized. This
trend has also been observed in the natural language processing (NLP)
literature where researchers have recently highlighted the importance of
well-informed search techniques<sup id="fnref3:26"><a class="footnote-ref" href="#fn:26">26</a></sup>.</p>
<p>Regardless of the decoding strategy, we observed improvements in mean
logP with iterations (<strong>Figure 5A</strong>) when using BBRT.
When optimizing for logP, a logP scoring function quickly discovers the
best scoring compounds while secondary scoring functions improve logP at
a slower rate and do not converge to the same scores (<strong>Figure 5A. Right</strong>). This trade-off highlights the role of conflicting molecular design objectives.</p>
<p>For <strong>Figure 5A</strong>, the standard deviation typically
decreased with iteration number <span><span class="MathJax_Preview">n</span><script type="math/tex">n</script></span>. Property values concentrate to a
certain range. With property values concentrating, it is reasonable to
question whether BBRT produces compounds with less diversity. In Figure
<strong>Figure 5B</strong> we show average pairwise diversity of translated
outputs per recursive iteration across 3 decoding strategies and observe
decay in diversity for logP. For the best performing decoding strategy,
the top-<span><span class="MathJax_Preview">5</span><script type="math/tex">5</script></span> sampler, diversity decays from about 0.86 after a single
translation to about 0.78 after <span><span class="MathJax_Preview">n=25</span><script type="math/tex">n=25</script></span> recursive translations. This
decay may be a product of the data -- higher logP values tend to be less
diverse than a random set of compounds. For QED (<strong>Figure 5B. Right</strong>), we observe limited decay. Differences indecay rate might be attributed to task variability: one being explorative and the other interpolative.</p>
<h3 id="interpretable-user-centric-optimization">Interpretable, user-centric optimization</h3>
<p>For project teams in drug design, the number of generated suggestions
can be a practical challenge to prioritization of experiments. We found
that interpretable paths of optimization facilitate user adoption.</p>
<p><strong>Molecular traces.</strong> BBRT generates a sequence of iterative, local
edits from an initial compound to an optimized one. We call these
optimization paths a <strong>molecular trace</strong>:</p>
<p><img src="../img/mol_rec.png" alt="bbrt-mol-rec" width="750"/>
<center><small><b>Figure 6:</b> <strong>(A)</strong>: Generated molecular trace by ranking intermediate outputs by the maximum pairwise Tanimoto similarity. <strong>(B)</strong>: An example of a molecular breakpoint. Alternative translations are considered from compound (2) each with its own design trade-offs.</small></center></p>
<p>We use the Min Delta Sim scoring function to generate traces that have the
minimum possible structural changes, while still improving logP. While
some graph-based approaches<sup id="fnref8:5"><a class="footnote-ref" href="#fn:5">5</a></sup> <sup id="fnref4:10"><a class="footnote-ref" href="#fn:10">10</a></sup> <sup id="fnref:45"><a class="footnote-ref" href="#fn:45">45</a></sup> can return valid,
intermediate graph states that are capable of being interrogated, we
liken our molecular traces to a sequence of Free-Wilson analysis<sup id="fnref:46"><a class="footnote-ref" href="#fn:46">46</a></sup> steps towards optimal molecules. Each step represents a local model built using molecular subgraphs with the biological activity
of molecules being described by linear summations of activity contributions of specific subgraphs. Consequently, this approach provides interpretability within the chemical space spanned by the subgraphs<sup id="fnref:47"><a class="footnote-ref" href="#fn:47">47</a></sup>.</p>
<p><strong>Molecular breakpoints.</strong> Molecular design requires choosing between
conflicting objectives. For example, while increased logP is correlated
with poor oral drug-like properties and rapid clearance from the body<sup id="fnref:48"><a class="footnote-ref" href="#fn:48">48</a></sup>, increasing QED might translate to a compound that is structurally dissimilar from the seed compound, which could result in an
inactive compound against a target. Our method allows users to
"debug" any step of the translation process and consider alternative
steps, similar to breakpoints in computer programs. In <strong>Figure 6B</strong>, we show an example from an BBRT-Seq2Seq model trained to optimize logP. Here, we revisit the translation from step (2)
to step (3) in <strong>Figure 6A</strong> by considering four alternatives picked from 100 stochastically decoded compounds. These alternatives require evaluating the trade-offs between logP, QED,
molecular weight, the number of rotational bonds, and chemical
similarity with compound (2).</p>
<h3 id="improving-secondary-properties-by-ranking">Improving secondary properties by ranking</h3>
<p>We consider secondary property optimization by ranking recursive outputs
using a scoring function. This function decides what compound is
propagated to the next recursive step. We apply BBRT to Seq2Seq modeling
(BBRT-Seq2Seq) and use the trained QED translator described in <a href="#molecule-generation-results">Section 5.1</a>. The inference task is to optimize QED and logP as the primary and secondary
properties respectively. We compare scoring outputs based on QED and
logP:</p>
<p><img src="../img/multiprop.png" alt="bbrt-multiprop" width="700"/>
<center><small><b>Figure 7:</b> Applying BBRT to multi-property optimization. QED is the primary target and logP is the secondary property. <strong>(A)</strong>: Average logP as a function of recursive iteration for three decoding strategies with primary and secondary property scoring functions. <strong>(B)</strong>: Average QED as a function of recursive iteration for three decoding strategies with primary and secondary property scoring functions.</small></center></p>
<p>In <strong>Figure 7A</strong>, we compute the average logP per
recursive iteration for a set of translated compounds across three
decoding strategies. Dotted lines optimize logP as the scoring function
while the solid lines optimize QED. For both scoring functions, we
report the maximum logP value generated. For all decoding strategies,
average logP reaches higher values under scoring by logP relative to
scoring by QED. In <strong>Figure 7B</strong>, we plot average QED values
using the same setup and observe that optimizing logP still
significantly improves the QED values of generated compounds. This
method also discovers the same maximum QED value as scoring by QED. This
improvement, however, has trade-offs in the limit for average QED values
generated. After <span><span class="MathJax_Preview">n=15</span><script type="math/tex">n=15</script></span> recursive iterations the <em>average</em> QED values of
the generated compounds under a logP scoring function converge to lower
values relative to QED values for compounds scored by QED for all three
decoding strategies. We repeat this experiment with JTNN and show
similar effects:</p>
<p><img src="../img/multiprop_g2g.png" alt="bbrt-multiprop-g2g" width="700"/>
<center><small><b>Figure 8:</b> Applying BBRT-JTNN to secondary property optimization. QED is the primary target and logP is the secondary property. <strong>(Left)</strong>: Average logP as a function of recursive iteration under two scoring functions---QED and logP. <strong>(Right)</strong>: Average QED as a function of recursive iteration for same two scoring functions.</small></center></p>
<p>Secondary property optimization by ranking extends to variables that are
at minimum loosely positively correlated. For QED optimization, the
average logP value for unoptimized QED compounds is <span><span class="MathJax_Preview">0.30 \pm{1.96}</span><script type="math/tex">0.30 \pm{1.96}</script></span>
while for optimized QED compounds the average logP value is
<span><span class="MathJax_Preview">0.79 \pm{1.45}</span><script type="math/tex">0.79 \pm{1.45}</script></span>. Additionally, QED compounds in the target range [0.9
1.0] in the training data had a positive correlation between its logP
and QED values (Spearman rank correlation; <span><span class="MathJax_Preview">\rho=0.07</span><script type="math/tex">\rho=0.07</script></span> <span><span class="MathJax_Preview">P &lt; 0.026</span><script type="math/tex">P < 0.026</script></span>).
This correlation did not hold for QED compounds in the range [0.7 0.8]
unoptimized QED compounds (<span><span class="MathJax_Preview">\rho=0.007</span><script type="math/tex">\rho=0.007</script></span>, <span><span class="MathJax_Preview">P&lt;0.8</span><script type="math/tex">P<0.8</script></span>).</p>
<h2 id="future-work">Future work</h2>
<p>We develop BBRT for molecular optimization. BBRT is a simple algorithm
that feeds the output of translation models back into the same model for
additional optimization. We apply BBRT to well-known models in the
literature and produce new state-of-the-art results for property
optimization tasks. We describe molecular traces and user-centric
optimization with molecular breakpoints. Finally, we show how BBRT can
be used for multi-property optimization. For future work, we will extend
BBRT to consider multiple translation paths simultaneously. Moreover, as
BBRT is limited by the construction of labeled training pairs, we plan
to extend translation models to low-resource settings, where property
annotations are expensive to collect.</p>
<h2 id="appendix">Appendix</h2>
<h3 id="a-recursive-penalized-logp-experiments">A. Recursive penalized logP experiments</h3>
<p><strong>Training details</strong>. For the Seq2Seq model, the hidden state dimension
is 500. We use a 2 layer bidirectional RNN encoder and 1 layer
unidirectional decoder with attention<sup id="fnref2:30"><a class="footnote-ref" href="#fn:30">30</a></sup>. The model was
trained using an Adam optimizer for 20 epochs with learning rate 0.001.
For the graph-based model, we used the implementation from Jin et al. (2019)<sup id="fnref15:3"><a class="footnote-ref" href="#fn:3">3</a></sup>,
which can be downloaded from <a href="https://github.com/wengong-jin/iclr19-graph2graph">https://github.com/wengong-jin/iclr19-graph2graph</a>.</p>
<p><strong>Property calculation</strong>. Penalized logP is calculated using the
implementation from You et al. (2018)<sup id="fnref3:38"><a class="footnote-ref" href="#fn:38">38</a></sup>. <a href="https://github.com/bowenliu16/rl_graph_generation">Their implementation</a> utilizes RDKit to
compute clogP and synthetic accessibility scores. QED scores are also
computed using RDKit.</p>
<h3 id="b-supplemental-experiments">B. Supplemental Experiments</h3>
<p><img src="../img/ranking_comparison_rg2g.png" alt="bbrt-rank-comparison-rg2g" width="700"/></p>
<p><center><small><b>Figure 9:</b> Comparison of scoring functions for BBRT-JTNN. Y-axis is mean logP
from 900 translations as a function of recursive iteration. Dotted lines denote non-recursive counterparts. "Rec. Inf." is synonymous with BBRT-JTNN.</small></center></p>
<p><img src="../img/toplogp_trace.png" alt="bbrt-top-logp-trace" width="700"/></p>
<p><center><small><b>Figure 10:</b> A molecular trace from optimizing logP using a logP scoring function.</small></center></p>
<p><img src="../img/edit_distance.png" alt="bbrt-edit-distance" width="700"/></p>
<p><center><small><b>Figure 11:</b> Comparing pairwise Levenshtein edit distances for generated sequences
after running BBRT under two different scoring functions (logP and
maximum pairwise Tanimoto similarity) and two different decoding
strategies (top-2 and top-5 sampling). Standard errors are reported from
a population of 900 sequences per iteration..</small></center></p>
<!-- ----

<object data="../img/bbrt_iclr2020.pdf" type="application/pdf" width="750px" height="800px">
    <embed src="../img/bbrt_iclr2020.pdf">
    </embed>
</object> -->

<div class="footnote">
<hr />
<ol>
<li id="fn:1">
<p>Sunghwan Kim, Paul A Thiessen, Evan E Bolton, Jie Chen, Gang Fu, Asta Gindulyte, Lianyi Han, Jane He, Siqian He, Benjamin A Shoemaker, and others. Pubchem substance and compound databases. <em>Nucleic acids research</em>, 44<span><span class="MathJax_Preview">D1</span><script type="math/tex">D1</script></span>:D1202–D1213, 2015.&#160;<a class="footnote-backref" href="#fnref:1" title="Jump back to footnote 1 in the text">&#8617;</a></p>
</li>
<li id="fn:2">
<p>Pavel G Polishchuk, Timur I Madzhidov, and Alexandre Varnek. Estimation of the size of drug-like chemical space based on gdb-17 data. <em>Journal of computer-aided molecular design</em>, 27<span><span class="MathJax_Preview">8</span><script type="math/tex">8</script></span>:675–679, 2013.&#160;<a class="footnote-backref" href="#fnref:2" title="Jump back to footnote 2 in the text">&#8617;</a></p>
</li>
<li id="fn:3">
<p>Wengong Jin, Kevin Yang, Regina Barzilay, and Tommi Jaakkola. Learning multimodal graph-to-graph translation for molecule optimization. In <em>International Conference on Learning Representations</em>. 2019.&#160;<a class="footnote-backref" href="#fnref:3" title="Jump back to footnote 3 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:3" title="Jump back to footnote 3 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:3" title="Jump back to footnote 3 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:3" title="Jump back to footnote 3 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:3" title="Jump back to footnote 3 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:3" title="Jump back to footnote 3 in the text">&#8617;</a><a class="footnote-backref" href="#fnref7:3" title="Jump back to footnote 3 in the text">&#8617;</a><a class="footnote-backref" href="#fnref8:3" title="Jump back to footnote 3 in the text">&#8617;</a><a class="footnote-backref" href="#fnref9:3" title="Jump back to footnote 3 in the text">&#8617;</a><a class="footnote-backref" href="#fnref10:3" title="Jump back to footnote 3 in the text">&#8617;</a><a class="footnote-backref" href="#fnref11:3" title="Jump back to footnote 3 in the text">&#8617;</a><a class="footnote-backref" href="#fnref12:3" title="Jump back to footnote 3 in the text">&#8617;</a><a class="footnote-backref" href="#fnref13:3" title="Jump back to footnote 3 in the text">&#8617;</a><a class="footnote-backref" href="#fnref14:3" title="Jump back to footnote 3 in the text">&#8617;</a><a class="footnote-backref" href="#fnref15:3" title="Jump back to footnote 3 in the text">&#8617;</a></p>
</li>
<li id="fn:4">
<p>Rafael Gómez-Bombarelli, Jennifer N Wei, David Duvenaud, José Miguel Hernández-Lobato, Benjamín Sánchez-Lengeling, Dennis Sheberla, Jorge Aguilera-Iparraguirre, Timothy D Hirzel, Ryan P Adams, and Alán Aspuru-Guzik. Automatic Chemical Design Using a Data-Driven Continuous Representation of Molecules. <em>ACS Central Science</em>, 4<span><span class="MathJax_Preview">2</span><script type="math/tex">2</script></span>:268–276, January 2018.&#160;<a class="footnote-backref" href="#fnref:4" title="Jump back to footnote 4 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:4" title="Jump back to footnote 4 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:4" title="Jump back to footnote 4 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:4" title="Jump back to footnote 4 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:4" title="Jump back to footnote 4 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:4" title="Jump back to footnote 4 in the text">&#8617;</a><a class="footnote-backref" href="#fnref7:4" title="Jump back to footnote 4 in the text">&#8617;</a><a class="footnote-backref" href="#fnref8:4" title="Jump back to footnote 4 in the text">&#8617;</a></p>
</li>
<li id="fn:5">
<p>Wengong Jin, Regina Barzilay, and Tommi Jaakkola. Junction tree variational autoencoder for molecular graph generation. In <em>International Conference on Machine Learning</em>, 2328–2337. 2018.&#160;<a class="footnote-backref" href="#fnref:5" title="Jump back to footnote 5 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:5" title="Jump back to footnote 5 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:5" title="Jump back to footnote 5 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:5" title="Jump back to footnote 5 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:5" title="Jump back to footnote 5 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:5" title="Jump back to footnote 5 in the text">&#8617;</a><a class="footnote-backref" href="#fnref7:5" title="Jump back to footnote 5 in the text">&#8617;</a><a class="footnote-backref" href="#fnref8:5" title="Jump back to footnote 5 in the text">&#8617;</a></p>
</li>
<li id="fn:6">
<p>Hanjun Dai, Yingtao Tian, Bo Dai, Steven Skiena, and Le Song. Syntax-directed variational autoencoder for structured data. In <em>International Conference on Learning Representations</em>. 2018.&#160;<a class="footnote-backref" href="#fnref:6" title="Jump back to footnote 6 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:6" title="Jump back to footnote 6 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:6" title="Jump back to footnote 6 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:6" title="Jump back to footnote 6 in the text">&#8617;</a></p>
</li>
<li id="fn:7">
<p>Yujia Li, Oriol Vinyals, Chris Dyer, Razvan Pascanu, and Peter Battaglia. Learning Deep Generative Models of Graphs. <em>arXiv.org</em>, March 2018. <a href="https://arxiv.org/abs/1803.03324v1">arXiv:1803.03324v1</a>.&#160;<a class="footnote-backref" href="#fnref:7" title="Jump back to footnote 7 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:7" title="Jump back to footnote 7 in the text">&#8617;</a></p>
</li>
<li id="fn:8">
<p>Matt J Kusner, Brooks Paige, and José Miguel Hernández-Lobato. Grammar variational autoencoder. In <em>International Conference on Machine Learning</em>, 1945–1954. 2017.&#160;<a class="footnote-backref" href="#fnref:8" title="Jump back to footnote 8 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:8" title="Jump back to footnote 8 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:8" title="Jump back to footnote 8 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:8" title="Jump back to footnote 8 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:8" title="Jump back to footnote 8 in the text">&#8617;</a><a class="footnote-backref" href="#fnref6:8" title="Jump back to footnote 8 in the text">&#8617;</a><a class="footnote-backref" href="#fnref7:8" title="Jump back to footnote 8 in the text">&#8617;</a></p>
</li>
<li id="fn:9">
<p>Mario Krenn, Florian Häse, AkshatKumar Nigam, Pascal Friederich, and Alán Aspuru-Guzik. SELFIES: a robust representation of semantically constrained graphs with an example application in chemistry. <em>arXiv.org</em>, May 2019. <a href="https://arxiv.org/abs/1905.13741v1">arXiv:1905.13741v1</a>.&#160;<a class="footnote-backref" href="#fnref:9" title="Jump back to footnote 9 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:9" title="Jump back to footnote 9 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:9" title="Jump back to footnote 9 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:9" title="Jump back to footnote 9 in the text">&#8617;</a><a class="footnote-backref" href="#fnref5:9" title="Jump back to footnote 9 in the text">&#8617;</a></p>
</li>
<li id="fn:10">
<p>Jiaxuan You, Rex Ying, Xiang Ren, William Hamilton, and Jure Leskovec. Graphrnn: generating realistic graphs with deep auto-regressive models. In <em>International Conference on Machine Learning</em>, 5694–5703. 2018.&#160;<a class="footnote-backref" href="#fnref:10" title="Jump back to footnote 10 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:10" title="Jump back to footnote 10 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:10" title="Jump back to footnote 10 in the text">&#8617;</a><a class="footnote-backref" href="#fnref4:10" title="Jump back to footnote 10 in the text">&#8617;</a></p>
</li>
<li id="fn:11">
<p>Ari Seff, Wenda Zhou, Farhan Damani, Abigail Doyle, and Ryan P Adams. Discrete Object Generation with Reversible Inductive Construction. <em>arXiv.org</em>, July 2019. <a href="https://arxiv.org/abs/1907.08268v1">arXiv:1907.08268v1</a>.&#160;<a class="footnote-backref" href="#fnref:11" title="Jump back to footnote 11 in the text">&#8617;</a></p>
</li>
<li id="fn:12">
<p>Mariya Popova, Olexandr Isayev, and Alexander Tropsha. Deep reinforcement learning for de novo drug design. <em>Science Advances</em>, 4<span><span class="MathJax_Preview">7</span><script type="math/tex">7</script></span>:eaap7885, July 2018.&#160;<a class="footnote-backref" href="#fnref:12" title="Jump back to footnote 12 in the text">&#8617;</a></p>
</li>
<li id="fn:13">
<p>Zhenpeng Zhou, Steven Kearnes, Li Li, Richard N Zare, and Patrick Riley. Optimization of Molecules via Deep Reinforcement Learning. <em>Scientific reports</em>, 9<span><span class="MathJax_Preview">1</span><script type="math/tex">1</script></span>:10752, July 2019.&#160;<a class="footnote-backref" href="#fnref:13" title="Jump back to footnote 13 in the text">&#8617;</a></p>
</li>
<li id="fn:14">
<p>Jonas Mueller, David Gifford, and Tommi Jaakkola. Sequence to better sequence: continuous revision of combinatorial structures. In <em>International Conference on Machine Learning</em>, 2536–2544. 2017.&#160;<a class="footnote-backref" href="#fnref:14" title="Jump back to footnote 14 in the text">&#8617;</a></p>
</li>
<li id="fn:15">
<p>Christian Kramer, Julian E Fuchs, Steven Whitebread, Peter Gedeck, and Klaus R Liedl. Matched Molecular Pair Analysis: Significance and the Impact of Experimental Uncertainty. <em>Journal of Medicinal Chemistry</em>, 57<span><span class="MathJax_Preview">9</span><script type="math/tex">9</script></span>:3786–3802, April 2014.&#160;<a class="footnote-backref" href="#fnref:15" title="Jump back to footnote 15 in the text">&#8617;</a></p>
</li>
<li id="fn:16">
<p>Samo Turk, Benjamin Merget, Friedrich Rippmann, and Simone Fulle. Coupling Matched Molecular Pairs with Machine Learning for Virtual Compound Optimization. <em>Journal of Chemical Information and Modeling</em>, 57<span><span class="MathJax_Preview">12</span><script type="math/tex">12</script></span>:3079–3085, dec 2017. <a href="https://doi.org/10.1021/acs.jcim.7b00298">doi:10.1021/acs.jcim.7b00298</a>.&#160;<a class="footnote-backref" href="#fnref:16" title="Jump back to footnote 16 in the text">&#8617;</a></p>
</li>
<li id="fn:17">
<p>Christian Tyrchan and Emma Evertsson. Matched Molecular Pair Analysis in Short: Algorithms, Applications and Limitations. <em>Computational and Structural Biotechnology Journal</em>, 15:86–90, jan 2017. <a href="https://doi.org/10.1016/j.csbj.2016.12.003">doi:10.1016/j.csbj.2016.12.003</a>.&#160;<a class="footnote-backref" href="#fnref:17" title="Jump back to footnote 17 in the text">&#8617;</a></p>
</li>
<li id="fn:18">
<p>Hugo Kubinyi. Free wilson analysis. theory, applications and its relationship to hansch analysis. <em>Quantitative Structure-Activity Relationships</em>, 7<span><span class="MathJax_Preview">3</span><script type="math/tex">3</script></span>:121–133, 1988.&#160;<a class="footnote-backref" href="#fnref:18" title="Jump back to footnote 18 in the text">&#8617;</a></p>
</li>
<li id="fn:19">
<p>Marco Tulio Ribeiro, Sameer Singh, and Carlos Guestrin. Why should i trust you?: explaining the predictions of any classifier. <em>Proceedings of the 22<sup>nd</sup> ACM SIGKDD International Conference on Knowledge Discovery and Data Mining - KDD ’16</em>, 2016.&#160;<a class="footnote-backref" href="#fnref:19" title="Jump back to footnote 19 in the text">&#8617;</a></p>
</li>
<li id="fn:20">
<p>Ilya Sutskever, Oriol Vinyals, and Quoc V. Le. Sequence to sequence learning with neural networks. In <em>Proceedings of the 27<sup>th</sup> International Conference on Neural Information Processing Systems - Volume 2</em>, NIPS'14, 3104–3112. Cambridge, MA, USA, 2014. MIT Press.&#160;<a class="footnote-backref" href="#fnref:20" title="Jump back to footnote 20 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:20" title="Jump back to footnote 20 in the text">&#8617;</a></p>
</li>
<li id="fn:21">
<p>Oriol Vinyals and Quoc Le. A Neural Conversational Model. <em>arXiv.org</em>, June 2015. <a href="https://arxiv.org/abs/1506.05869v3">arXiv:1506.05869v3</a>.&#160;<a class="footnote-backref" href="#fnref:21" title="Jump back to footnote 21 in the text">&#8617;</a></p>
</li>
<li id="fn:22">
<p>Oriol Vinyals, Alexander Toshev, Samy Bengio, and Dumitru Erhan. Show and tell: a neural image caption generator. In <em>Proceedings of the IEEE conference on computer vision and pattern recognition</em>, 3156–3164. 2015.&#160;<a class="footnote-backref" href="#fnref:22" title="Jump back to footnote 22 in the text">&#8617;</a></p>
</li>
<li id="fn:23">
<p>Ari Holtzman, Jan Buys, Maxwell Forbes, and Yejin Choi. The Curious Case of Neural Text Degeneration. <em>arXiv.org</em>, April 2019. <a href="https://arxiv.org/abs/1904.09751v1">arXiv:1904.09751v1</a>.&#160;<a class="footnote-backref" href="#fnref:23" title="Jump back to footnote 23 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:23" title="Jump back to footnote 23 in the text">&#8617;</a></p>
</li>
<li id="fn:24">
<p>Jiwei Li and Dan Jurafsky. Mutual Information and Diverse Decoding Improve Neural Machine Translation. <em>arXiv.org</em>, January 2016. <a href="https://arxiv.org/abs/1611.08562v2">arXiv:1611.08562v2</a>.&#160;<a class="footnote-backref" href="#fnref:24" title="Jump back to footnote 24 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:24" title="Jump back to footnote 24 in the text">&#8617;</a></p>
</li>
<li id="fn:25">
<p>Ashwin K Vijayakumar, Michael Cogswell, Ramprasaath R Selvaraju, Qing Sun, Stefan Lee, David Crandall, and Dhruv Batra. Diverse Beam Search for Improved Description of Complex Scenes. <em>Thirty-Second AAAI Conference on Artificial Intelligence</em>, April 2018.&#160;<a class="footnote-backref" href="#fnref:25" title="Jump back to footnote 25 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:25" title="Jump back to footnote 25 in the text">&#8617;</a></p>
</li>
<li id="fn:26">
<p>Ilya Kulikov, Alexander H Miller, Kyunghyun Cho, and Jason Weston. Importance of a Search Strategy in Neural Dialogue Modelling. <em>arXiv.org</em>, November 2018. <a href="https://arxiv.org/abs/1811.00907v2">arXiv:1811.00907v2</a>.&#160;<a class="footnote-backref" href="#fnref:26" title="Jump back to footnote 26 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:26" title="Jump back to footnote 26 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:26" title="Jump back to footnote 26 in the text">&#8617;</a></p>
</li>
<li id="fn:27">
<p>Angela Fan, Mike Lewis, and Yann N Dauphin. Hierarchical Neural Story Generation. <em>ACL</em>, pages 889–898, 2018.&#160;<a class="footnote-backref" href="#fnref:27" title="Jump back to footnote 27 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:27" title="Jump back to footnote 27 in the text">&#8617;</a></p>
</li>
<li id="fn:28">
<p>Sam Wiseman, Stuart Shieber, and Alexander Rush. Learning neural templates for text generation. In <em>Proceedings of the 2018 Conference on Empirical Methods in Natural Language Processing</em>, 3174–3187. 2018.&#160;<a class="footnote-backref" href="#fnref:28" title="Jump back to footnote 28 in the text">&#8617;</a></p>
</li>
<li id="fn:29">
<p>Shiqi Shen, Yong Cheng, Zhongjun He, Wei He, Hua Wu, Maosong Sun, and Yang Liu. Minimum risk training for neural machine translation. In <em>Proceedings of the 54<sup>th</sup> Annual Meeting of the Association for Computational Linguistics <span><span class="MathJax_Preview">Volume 1: Long Papers</span><script type="math/tex">Volume 1: Long Papers</script></span></em>, 1683–1692. 2016.&#160;<a class="footnote-backref" href="#fnref:29" title="Jump back to footnote 29 in the text">&#8617;</a></p>
</li>
<li id="fn:30">
<p>Dzmitry Bahdanau, Philemon Brakel, Kelvin Xu, Anirudh Goyal, Ryan Lowe, Joelle Pineau, Aaron Courville, and Yoshua Bengio. An Actor-Critic Algorithm for Sequence Prediction. In <em>International Conference on Learning Representations</em>. 2017.&#160;<a class="footnote-backref" href="#fnref:30" title="Jump back to footnote 30 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:30" title="Jump back to footnote 30 in the text">&#8617;</a></p>
</li>
<li id="fn:31">
<p>Dzmitry Bahdanau, Kyunghyun Cho, and Yoshua Bengio. Neural Machine Translation by Jointly Learning to Align and Translate. <em>arXiv.org</em>, September 2014. <a href="https://arxiv.org/abs/1409.0473v7">arXiv:1409.0473v7</a>.&#160;<a class="footnote-backref" href="#fnref:31" title="Jump back to footnote 31 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:31" title="Jump back to footnote 31 in the text">&#8617;</a></p>
</li>
<li id="fn:32">
<p>Sean Welleck, Kianté Brantley, Hal Daumé Iii, and Kyunghyun Cho. Non-monotonic sequential text generation. In <em>International Conference on Machine Learning</em>, 6716–6726. 2019.&#160;<a class="footnote-backref" href="#fnref:32" title="Jump back to footnote 32 in the text">&#8617;</a></p>
</li>
<li id="fn:33">
<p>Alex Graves. Sequence Transduction with Recurrent Neural Networks. <em>arXiv.org</em>, November 2012. <a href="https://arxiv.org/abs/1211.3711v1">arXiv:1211.3711v1</a>.&#160;<a class="footnote-backref" href="#fnref:33" title="Jump back to footnote 33 in the text">&#8617;</a></p>
</li>
<li id="fn:34">
<p>Nicolas Boulanger-Lewandowski, Yoshua Bengio, and Pascal Vincent. Audio Chord Recognition with Recurrent Neural Networks. <em>ISMIR</em>, 2013.&#160;<a class="footnote-backref" href="#fnref:34" title="Jump back to footnote 34 in the text">&#8617;</a></p>
</li>
<li id="fn:35">
<p>Sepp Hochreiter and Jürgen Schmidhuber. Long short-term memory. <em>Neural Computation</em>, 9<span><span class="MathJax_Preview">8</span><script type="math/tex">8</script></span>:1735–1780, November 1997.&#160;<a class="footnote-backref" href="#fnref:35" title="Jump back to footnote 35 in the text">&#8617;</a></p>
</li>
<li id="fn:36">
<p>David Rogers and Mathew Hahn. Extended-connectivity fingerprints. <em>Journal of chemical information and modeling</em>, 50<span><span class="MathJax_Preview">5</span><script type="math/tex">5</script></span>:742–754, 2010.&#160;<a class="footnote-backref" href="#fnref:36" title="Jump back to footnote 36 in the text">&#8617;</a></p>
</li>
<li id="fn:37">
<p>John J Irwin, Teague Sterling, Michael M Mysinger, Erin S Bolstad, and Ryan G Coleman. ZINC: A Free Tool to Discover Chemistry for Biology. <em>Journal of Chemical Information and Modeling</em>, 52<span><span class="MathJax_Preview">7</span><script type="math/tex">7</script></span>:1757–1768, 2012.&#160;<a class="footnote-backref" href="#fnref:37" title="Jump back to footnote 37 in the text">&#8617;</a></p>
</li>
<li id="fn:38">
<p>Jiaxuan You, Bowen Liu, Zhitao Ying, Vijay Pande, and Jure Leskovec. Graph convolutional policy network for goal-directed molecular graph generation. In <em>Advances in Neural Information Processing Systems</em>, 6410–6421. 2018.&#160;<a class="footnote-backref" href="#fnref:38" title="Jump back to footnote 38 in the text">&#8617;</a><a class="footnote-backref" href="#fnref2:38" title="Jump back to footnote 38 in the text">&#8617;</a><a class="footnote-backref" href="#fnref3:38" title="Jump back to footnote 38 in the text">&#8617;</a></p>
</li>
<li id="fn:39">
<p>G Richard Bickerton, Gaia V Paolini, Jérémy Besnard, Sorel Muresan, and Andrew L Hopkins. Quantifying the chemical beauty of drugs. <em>Nature chemistry</em>, 4<span><span class="MathJax_Preview">2</span><script type="math/tex">2</script></span>:90, 2012.&#160;<a class="footnote-backref" href="#fnref:39" title="Jump back to footnote 39 in the text">&#8617;</a></p>
</li>
<li id="fn:40">
<p>Wengong Jin, Regina Barzilay, and Tommi Jaakkola. Multi-resolution Autoregressive Graph-to-Graph Translation for Molecules. <em>arXiv.org</em>, June 2019. <a href="https://arxiv.org/abs/1907.11223v1">arXiv:1907.11223v1</a>.&#160;<a class="footnote-backref" href="#fnref:40" title="Jump back to footnote 40 in the text">&#8617;</a></p>
</li>
<li id="fn:41">
<p>Mark Ashton, John Barnard, Florence Casset, Michael Charlton, Geoffrey Downs, Dominique Gorse, John Holliday, Roger Lahana, and Peter Willett. Identification of diverse database subsets using Property-Based and Fragment-Based molecular descriptions. <em>Quant. Struct.-Act.Relat.</em>, 21<span><span class="MathJax_Preview">6</span><script type="math/tex">6</script></span>:598–604, December 2002.&#160;<a class="footnote-backref" href="#fnref:41" title="Jump back to footnote 41 in the text">&#8617;</a></p>
</li>
<li id="fn:42">
<p>Gabriel Lima Guimaraes, Benjamín Sánchez-Lengeling, Carlos Outeiral, Pedro Luis Cunha Farias, and Alán Aspuru-Guzik. Objective-Reinforced Generative Adversarial Networks <span><span class="MathJax_Preview">ORGAN</span><script type="math/tex">ORGAN</script></span> for Sequence Generation Models. <em>arXiv.org</em>, May 2017. <a href="https://arxiv.org/abs/1705.10843v3">arXiv:1705.10843v3</a>.&#160;<a class="footnote-backref" href="#fnref:42" title="Jump back to footnote 42 in the text">&#8617;</a></p>
</li>
<li id="fn:43">
<p>David Weininger. Smiles, a chemical language and information system. 1. introduction to methodology and encoding rules. <em>Journal of chemical information and computer sciences</em>, 28<span><span class="MathJax_Preview">1</span><script type="math/tex">1</script></span>:31–36, 1988.&#160;<a class="footnote-backref" href="#fnref:43" title="Jump back to footnote 43 in the text">&#8617;</a></p>
</li>
<li id="fn:44">
<p>Ksenia Korovina, Sailun Xu, Kirthevasan Kandasamy, Willie Neiswanger, Barnabas Poczos, Jeff Schneider, and Eric P Xing. ChemBO: Bayesian Optimization of Small Organic Molecules with Synthesizable Recommendations. <em>arXiv.org</em>, August 2019. <a href="https://arxiv.org/abs/1908.01425v2">arXiv:1908.01425v2</a>.&#160;<a class="footnote-backref" href="#fnref:44" title="Jump back to footnote 44 in the text">&#8617;</a></p>
</li>
<li id="fn:45">
<p>Steven Kearnes, Li Li, and Patrick Riley. Decoding Molecular Graph Embeddings with Reinforcement Learning. <em>arXiv.org</em>, April 2019. <a href="https://arxiv.org/abs/1904.08915v2">arXiv:1904.08915v2</a>.&#160;<a class="footnote-backref" href="#fnref:45" title="Jump back to footnote 45 in the text">&#8617;</a></p>
</li>
<li id="fn:46">
<p>Spencer M Free and James W Wilson. A Mathematical Contribution to Structure-Activity Studies. <em>Journal of Medicinal Chemistry</em>, 7:395–399, July 1964.&#160;<a class="footnote-backref" href="#fnref:46" title="Jump back to footnote 46 in the text">&#8617;</a></p>
</li>
<li id="fn:47">
<p>Mats Eriksson, Hongming Chen, Lars Carlsson, J. Willem M. Nissink, John G. Cumming, and Ingemar Nilsson. Beyond the Scope of Free-Wilson Analysis. 2: Can Distance Encoded R-Group Fingerprints Provide Interpretable Nonlinear Models? <em>Journal of Chemical Information and Modeling</em>, 54<span><span class="MathJax_Preview">4</span><script type="math/tex">4</script></span>:1117–1128, 2014. <a href="https://doi.org/10.1021/ci500075q">doi:10.1021/ci500075q</a>.&#160;<a class="footnote-backref" href="#fnref:47" title="Jump back to footnote 47 in the text">&#8617;</a></p>
</li>
<li id="fn:48">
<p>Thomas Ryckmans, Martin P. Edwards, Val A. Horne, Ana Monica Correia, Dafydd R. Owen, Lisa R. Thompson, Isabelle Tran, Michelle F. Tutt, and Tim Young. Rapid assessment of a novel series of selective CB2 agonists using parallel synthesis protocols: A Lipophilic Efficiency <span><span class="MathJax_Preview">LipE</span><script type="math/tex">LipE</script></span> analysis. <em>Bioorganic and Medicinal Chemistry Letters</em>, 2009. <a href="https://doi.org/10.1016/j.bmcl.2009.05.062">doi:10.1016/j.bmcl.2009.05.062</a>.&#160;<a class="footnote-backref" href="#fnref:48" title="Jump back to footnote 48 in the text">&#8617;</a></p>
</li>
</ol>
</div>
                
              
              
                


              
            </article>
          </div>
        </div>
      </main>
      
        
<footer class="md-footer">
  
    <nav class="md-footer__inner md-grid" aria-label="Footer">
      
        <a href=".." class="md-footer__link md-footer__link--prev" rel="prev">
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11h12z"/></svg>
          </div>
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Previous
              </span>
              Black box recursive translations for molecular optimization
            </div>
          </div>
        </a>
      
      
        <a href="../05-07-19-adaptive-sampling/" class="md-footer__link md-footer__link--next" rel="next">
          <div class="md-footer__title">
            <div class="md-ellipsis">
              <span class="md-footer__direction">
                Next
              </span>
              On conditioning by adaptive sampling
            </div>
          </div>
          <div class="md-footer__button md-icon">
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M4 11v2h12l-5.5 5.5 1.42 1.42L19.84 12l-7.92-7.92L10.5 5.5 16 11H4z"/></svg>
          </div>
        </a>
      
    </nav>
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-footer-copyright">
        
          <div class="md-footer-copyright__highlight">
            Copyright &copy; 2020 Stephen Ra
          </div>
        
        Made with
        <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
          Material for MkDocs
        </a>
      </div>
      
  <div class="md-footer-social">
    
      
      
      <a href="mailto:stephen.ra@pfizer.com" target="_blank" rel="noopener" title="" class="md-footer-social__link">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path d="M494.586 164.516c-4.697-3.883-111.723-89.95-135.251-108.657C337.231 38.191 299.437 0 256 0c-43.205 0-80.636 37.717-103.335 55.859-24.463 19.45-131.07 105.195-135.15 108.549A48.004 48.004 0 0 0 0 201.485V464c0 26.51 21.49 48 48 48h416c26.51 0 48-21.49 48-48V201.509a48 48 0 0 0-17.414-36.993zM464 458a6 6 0 0 1-6 6H54a6 6 0 0 1-6-6V204.347c0-1.813.816-3.526 2.226-4.665 15.87-12.814 108.793-87.554 132.364-106.293C200.755 78.88 232.398 48 256 48c23.693 0 55.857 31.369 73.41 45.389 23.573 18.741 116.503 93.493 132.366 106.316a5.99 5.99 0 0 1 2.224 4.663V458zm-31.991-187.704c4.249 5.159 3.465 12.795-1.745 16.981-28.975 23.283-59.274 47.597-70.929 56.863C336.636 362.283 299.205 400 256 400c-43.452 0-81.287-38.237-103.335-55.86-11.279-8.967-41.744-33.413-70.927-56.865-5.21-4.187-5.993-11.822-1.745-16.981l15.258-18.528c4.178-5.073 11.657-5.843 16.779-1.726 28.618 23.001 58.566 47.035 70.56 56.571C200.143 320.631 232.307 352 256 352c23.602 0 55.246-30.88 73.41-45.389 11.994-9.535 41.944-33.57 70.563-56.568 5.122-4.116 12.601-3.346 16.778 1.727l15.258 18.526z"/></svg>
      </a>
    
      
      
        
        
      
      <a href="https://github.com/stephenra" target="_blank" rel="noopener" title="github.com" class="md-footer-social__link">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 480 512"><path d="M186.1 328.7c0 20.9-10.9 55.1-36.7 55.1s-36.7-34.2-36.7-55.1 10.9-55.1 36.7-55.1 36.7 34.2 36.7 55.1zM480 278.2c0 31.9-3.2 65.7-17.5 95-37.9 76.6-142.1 74.8-216.7 74.8-75.8 0-186.2 2.7-225.6-74.8-14.6-29-20.2-63.1-20.2-95 0-41.9 13.9-81.5 41.5-113.6-5.2-15.8-7.7-32.4-7.7-48.8 0-21.5 4.9-32.3 14.6-51.8 45.3 0 74.3 9 108.8 36 29-6.9 58.8-10 88.7-10 27 0 54.2 2.9 80.4 9.2 34-26.7 63-35.2 107.8-35.2 9.8 19.5 14.6 30.3 14.6 51.8 0 16.4-2.6 32.7-7.7 48.2 27.5 32.4 39 72.3 39 114.2zm-64.3 50.5c0-43.9-26.7-82.6-73.5-82.6-18.9 0-37 3.4-56 6-14.9 2.3-29.8 3.2-45.1 3.2-15.2 0-30.1-.9-45.1-3.2-18.7-2.6-37-6-56-6-46.8 0-73.5 38.7-73.5 82.6 0 87.8 80.4 101.3 150.4 101.3h48.2c70.3 0 150.6-13.4 150.6-101.3zm-82.6-55.1c-25.8 0-36.7 34.2-36.7 55.1s10.9 55.1 36.7 55.1 36.7-34.2 36.7-55.1-10.9-55.1-36.7-55.1z"/></svg>
      </a>
    
      
      
        
        
      
      <a href="https://twitter.com/stephenrra" target="_blank" rel="noopener" title="twitter.com" class="md-footer-social__link">
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512"><path d="M459.37 151.716c.325 4.548.325 9.097.325 13.645 0 138.72-105.583 298.558-298.558 298.558-59.452 0-114.68-17.219-161.137-47.106 8.447.974 16.568 1.299 25.34 1.299 49.055 0 94.213-16.568 130.274-44.832-46.132-.975-84.792-31.188-98.112-72.772 6.498.974 12.995 1.624 19.818 1.624 9.421 0 18.843-1.3 27.614-3.573-48.081-9.747-84.143-51.98-84.143-102.985v-1.299c13.969 7.797 30.214 12.67 47.431 13.319-28.264-18.843-46.781-51.005-46.781-87.391 0-19.492 5.197-37.36 14.294-52.954 51.655 63.675 129.3 105.258 216.365 109.807-1.624-7.797-2.599-15.918-2.599-24.04 0-57.828 46.782-104.934 104.934-104.934 30.213 0 57.502 12.67 76.67 33.137 23.715-4.548 46.456-13.32 66.599-25.34-7.798 24.366-24.366 44.833-46.132 57.827 21.117-2.273 41.584-8.122 60.426-16.243-14.292 20.791-32.161 39.308-52.628 54.253z"/></svg>
      </a>
    
  </div>

    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    <script id="__config" type="application/json">{"base": "..", "features": [], "translations": {"clipboard.copy": "Copy to clipboard", "clipboard.copied": "Copied to clipboard", "search.config.lang": "en", "search.config.pipeline": "trimmer, stopWordFilter", "search.config.separator": "[\\s\\-]+", "search.placeholder": "Search", "search.result.placeholder": "Type to start searching", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.term.missing": "Missing"}, "search": "../assets/javascripts/workers/search.fb4a9340.min.js", "version": null}</script>
    
    
      <script src="../assets/javascripts/bundle.ca5457b8.min.js"></script>
      
        <script src="../js/extra.js"></script>
      
        <script src="../js/mathjax-config.js"></script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.0/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
      
    
  </body>
</html>